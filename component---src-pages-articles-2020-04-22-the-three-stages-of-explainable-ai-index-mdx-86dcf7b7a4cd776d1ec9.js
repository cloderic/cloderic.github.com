(window.webpackJsonp=window.webpackJsonp||[]).push([[44],{"25dN":function(e,a,t){var n=t("XKFU");n(n.S,"Object",{is:t("g6HL")})},CyHz:function(e,a,t){var n=t("XKFU");n(n.S,"Math",{sign:t("lvtm")})},FLlr:function(e,a,t){var n=t("XKFU");n(n.P,"String",{repeat:t("l0Rn")})},fe0W:function(e,a,t){"use strict";var n=t("wTIg"),i=t("+e4l"),o=t("ExVU"),s=t("Bl7J"),r=t("iCNy"),l=t("PJDU"),c=(t("q1tI"),t("XNvW")),d=t("qKvR"),p=Object(n.a)("article",{target:"e1g54axm0"})("background:",c.BG_COLOR_1_GRADIENT,";z-index:10;",c.BOX_SHADOW,";p{line-height:1.7;font-size:1.1em;}h1,h2,h3,h4,h5,h6{font-weight:500;}h2,h3,hr,header.articleHeader{margin:2em 0;border:none;padding-bottom:0.5em;border-bottom:1px ",c.PRIMARY," solid;}"),h=Object(n.a)(i.a,{target:"e1g54axm1"})({name:"1jq247e",styles:"padding:2rem 0.5rem;"});a.a=function(e){var a=e.children,t=e.pageContext,n=e.location,i=t.frontmatter.date&&o.DateTime.fromISO(t.frontmatter.date),c=t.frontmatter.last_update&&o.DateTime.fromISO(t.frontmatter.last_update);return Object(d.d)(s.a,null,Object(d.d)(l.a,{title:t.frontmatter.title,publicationDate:i&&i.toJSDate(),path:n.pathname,updateDate:c&&c.toJSDate()}),Object(d.d)(p,null,Object(d.d)(h,null,Object(d.d)("header",{className:"articleHeader"},Object(d.d)(r.a,null),Object(d.d)("h1",null,t.frontmatter.title),i&&Object(d.d)("p",null,Object(d.d)("small",null,"Published on"," ",Object(d.d)("time",{dateTime:i.toISODate()},i.toFormat("yyyy/MM/dd")),"."))),a,Object(d.d)("footer",{className:"articleFooter"},Object(d.d)(r.a,null),c&&Object(d.d)("p",null,Object(d.d)("small",null,"Last updated on"," ",Object(d.d)("time",{dateTime:c.toISODate()},c.toFormat("yyyy/MM/dd")),"."))))))}},hLT2:function(e,a,t){var n=t("XKFU");n(n.S,"Math",{trunc:function(e){return(e>0?Math.floor:Math.ceil)(e)}})},iCNy:function(e,a,t){"use strict";t("jm62"),t("ioFf"),t("rGqo"),t("yt8O"),t("Btvt"),t("RW0V"),t("0mN4");var n=t("wTIg"),i=t("wwW9"),o=t("Aw06"),s=t("q1tI"),r=t("9eSz"),l=t.n(r),c=t("IP2g"),d=t("wHSu"),p=t("qKvR");function h(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);a&&(n=n.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,n)}return t}function m(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?h(Object(t),!0).forEach((function(a){u(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):h(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}function u(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}var b=Object(n.a)(o.a,{target:"e1gx71j0"})({name:"1cpgv1a",styles:"display:inline-flex;& > .mars{margin-inline-start:0.2em;transition:transform 0.3s;}&:hover > .mars{transform:scale(1.2) rotate(-10deg);}"});a.a=function(){var e=i.data.mars,a=Object(s.useMemo)((function(){return m(m({},e.childImageSharp.fixed),{},{height:"1em",width:"1em"})}),[e]);return Object(p.d)(b,{href:"/",title:"Back to the home page"},Object(p.d)(c.a,{icon:d.a}),Object(p.d)(l.a,{fixed:a,className:"mars"}))}},lvtm:function(e,a){e.exports=Math.sign||function(e){return 0==(e=+e)||e!=e?e:e<0?-1:1}},wwW9:function(e){e.exports=JSON.parse('{"data":{"mars":{"childImageSharp":{"fixed":{"base64":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAACXBIWXMAAC4jAAAuIwF4pT92AAAFW0lEQVQ4y1WUe0zTVxTHf79fWxBKoU8pLS0U6IM+LM+W8ipUBiIywedEdBFkFpWNIWpwqIg6xRgSfOEjczpUVByKyswgZjr/UIgTcAPdFt18Ydyy/bclFHa/+7Wgcyc5uTf33vPJ99x7z6FGtxVQo+sdlNeebs6jQU0aAPr+xtm2ax9kuzuWZza0lWQ0nCtxuvsqcm3ePe+ZYNYvFGT65p3ZcdTZAidFjdam+ADPNrmYKRb1aL2rrL/CMdhbkoDuRfH4sjge5wpn4HSeGR05JtyYHTf4dVF62evz7TNtvtjPUkyTCy/qs30Lr/aXSp/Vpvb+uTYOA0vZwAVG8u18w3h/UYzndkGU50q2ZrwtJYJcS1HjmwwdLrvierqXL5D5oFkJk4Jebn3HJ/lVS4n0RY19+K9qC16uNnh++zB2YnSNiTypMJOfSo1koFhPvsrRkjMODTkap5poNYV5TsVH4myqaeTsonelXsbJrCSa2jsl+3mto/fvGgteVZvGft9oJaMbEvG42kYeVyWTnyuS8P1iC+4W6Ml3MyPIDbsCHRY5OaibPnbMpMTBRF0P9bY92pC14o8qK55XGjxP11jIi9okMrA6nYyszcCvVWl4sNKB/sXJGCk24mG+BgOZ4eRmfCjpMkjJCY3I06qTo96oKXvzmn3lyYN3FhrwQ6lh4seVVtwuSyXDa9LxpDoTD92Z6F+WgTsLbBiYa8ZIgRaDThVuJrLAWAnORIsm9qmEWKWQDFHmQpr6pS7Hdn6eFadydewDGMntpQmkc4kTw+509JW70F2Si6vznLhTlIShOSbcz4tBX4YKNxJDcSVWQtqjhKRJEYz3xAISyw9Ioe6VO9zHZ5nRlRM1fq9YR3rnxZOL89Nwa0k6js7NRfOcWWjOzcK53GQM52tx16nGzeQw9FpluGQQk7bIENIkF4wXBgdCyuO6qYsL7Q0HXCbstkV7umZqSe9sM9rzk3F5jg2fz0rDpy4nqlIzcTjdir6sSNxyhON6khxdJilORwlxRBWMeinfYwnwA0XTjVT7XHtDU5oR62ZEeQ7bY8hFpx4HMqw47oxDh8uKzmwLDjjMqLPGYqtZgxajAlfNMpzXi3FEHYwmeRBWCAM8IVwuW2T0dqq7KMXdbNdjrV49vsmkIc3x0eRQkhbHbDq02bU4b4vCpWQ1OuKUaNAqUBIux44IMU5GhqAxNIh8LA4giQF+4zTNgKHoSur6omzbQTa4JkZJPtKqyHp9BNkaq8YukxqtlnAcNSvREqvAFwYZzujE2KYSYel0IepCBfDCHIH+ZBqH41VHeAyTQv3DfpvODOPgCaMcO6NDJzbHhOGTKDmpjw5FQ/R0bNHI0BApxR61GHvZ79EgF2ClJAiLQwJJPt8fEh53goWx3YIeculCfVVHXXHFl11gU9uvlXkOR0lIK+stGgnZyaZWpxRhlVyIZbIQlEoEWCjkI08wqUzB4xI2TY8XyNB0+f+qZV+qoWePPgy7lMKxPREisj1ciM1KIdkkDyY1MgFKREGwC/hEHxhAwv39IOVyCIemx3zqaPq/0jvkMPpknnTFybZow0bqVWLUKISeytCQiVUyAamSBpFKUSBKgqchh+9PTP48IuAwbJqUZwr2gMPh+DoOw5oPusOs9k1q1RJpqUzQs0TER3FIIApZLwoOGM/h+3kS/HkeFZczLqBp8NiK5bBO01QPh8v4YCx4ErYvKcY3rtOGvWmw7N2Uh3E5QzIOAzHD4DWE8YJoivBoeoj31p0xUzAWSlG7LWqqRinybbyvENFvXat37m3nbtYbGZpq9KNp9zQOnWKIlrw5xzCvYRTFpk79C0CdTqEKIXTrAAAAAElFTkSuQmCC","width":100,"height":100,"src":"/static/9381c89176963851c7ecba1737a9acb8/65e33/mars.png","srcSet":"/static/9381c89176963851c7ecba1737a9acb8/65e33/mars.png 1x,\\n/static/9381c89176963851c7ecba1737a9acb8/6d161/mars.png 1.5x,\\n/static/9381c89176963851c7ecba1737a9acb8/69585/mars.png 2x"}}}}}')},x2vJ:function(e,a,t){"use strict";t.r(a),t.d(a,"pageQuery",(function(){return r})),t.d(a,"_frontmatter",(function(){return l})),t.d(a,"default",(function(){return p}));t("91GP"),t("rGqo"),t("yt8O"),t("Btvt"),t("RW0V"),t("q1tI");var n=t("7ljp"),i=t("fe0W"),o=t("2C6Z");t("qKvR");function s(){return(s=Object.assign||function(e){for(var a=1;a<arguments.length;a++){var t=arguments[a];for(var n in t)Object.prototype.hasOwnProperty.call(t,n)&&(e[n]=t[n])}return e}).apply(this,arguments)}var r="2495968799",l={},c={pageQuery:r,_frontmatter:l},d=i.a;function p(e){var a=e.components,t=function(e,a){if(null==e)return{};var t,n,i={},o=Object.keys(e);for(n=0;n<o.length;n++)t=o[n],a.indexOf(t)>=0||(i[t]=e[t]);return i}(e,["components"]);return Object(n.mdx)(d,s({},c,t,{components:a,mdxType:"MDXLayout"}),Object(n.mdx)(o.a,{title:"The three stages of Explainable AI",src:t.data.slides.publicURL,start:0,end:1,noPagination:!0,noControls:!0,mdxType:"Pdf"}),Object(n.mdx)("blockquote",null,Object(n.mdx)("p",{parentName:"blockquote"},"This work matured throughout 2019 in\n",Object(n.mdx)("a",s({parentName:"p"},{href:"#history"}),"various conferences, round tables and workshops"),", it was formalized\nearly 2020 to be\n",Object(n.mdx)("a",s({parentName:"p"},{href:"#paper-published-in-the-proceedings-of-the-humains-et-ia-hia-workshop-of-the-20th-extraction-et-gestion-des-connaissances-egc-conference-in-2020"}),"published in the proceedings of EGC"),".")),Object(n.mdx)("p",null,"The explainability of AI has become a major concern for AI builders and users,\nespecially in the enterprise world. As AIs have more and more impact on the\ndaily operations of businesses, trust, acceptance, accountability and\ncertifiability become requirements for any deployment at a large scale."),Object(n.mdx)("h3",{id:"explainability-a-key-part-of-the-ai-industry-strategy",style:{position:"relative"}},Object(n.mdx)("a",s({parentName:"h3"},{href:"#explainability-a-key-part-of-the-ai-industry-strategy","aria-label":"explainability a key part of the ai industry strategy permalink",className:"anchor before"}),Object(n.mdx)("svg",s({parentName:"a"},{"aria-hidden":"true",height:"20",version:"1.1",viewBox:"0 0 16 16",width:"20"}),Object(n.mdx)("path",s({parentName:"svg"},{fillRule:"evenodd",d:"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"})))),"Explainability, a key part of the AI industry strategy"),Object(n.mdx)("p",null,"Explainable AI (XAI), as a field, was popularized by the\n",Object(n.mdx)("a",s({parentName:"p"},{href:"https://www.darpa.mil/attachments/XAIProgramUpdate.pdf"}),"eponymous DARPA program"),"\nlaunched in 2017, with the goal of creating a suite of machine learning\ntechniques that produce more ",Object(n.mdx)("em",{parentName:"p"},'"explainable"')," models while maintaining a high\nlevel of learning performance, thus enabling human users to understand, trust\nand effectively manage the emerging generation of AIs."),Object(n.mdx)(o.a,{title:"The three stages of Explainable AI - A key part of the AI industry strategy",src:t.data.slides.publicURL,start:1,end:13,noLinks:!0,noTitle:!0,mdxType:"Pdf"}),Object(n.mdx)("p",null,"Over the past 5 years, explainability has become an important part of the AI\ndiscussions and strategies. Everyone talks about it:"),Object(n.mdx)("ul",null,Object(n.mdx)("li",{parentName:"ul"},"Research institutions, such as DARPA, in the US, or\n",Object(n.mdx)("a",s({parentName:"li"},{href:"https://www.inria.fr/actualite/actualites-inria/intelligence-artificielle-les-defis-actuels-et-l-action-d-inria"}),"INRIA"),",\nin France, and even governments from the\n",Object(n.mdx)("a",s({parentName:"li"},{href:"https://obamawhitehouse.archives.gov/sites/default/files/whitehouse_files/microsites/ostp/NSTC/preparing_for_the_future_of_ai.pdf"}),"Obama administration"),"\nto Macron's government through the\n",Object(n.mdx)("a",s({parentName:"li"},{href:"https://www.aiforhumanity.fr/pdfs/9782111457089_Rapport_Villani_accessible.pdf"}),"Villani report"),"\nmakes it an objective ;"),Object(n.mdx)("li",{parentName:"ul"},"Big tech corporations, publish open source tools libraries such as\n",Object(n.mdx)("a",s({parentName:"li"},{href:"https://github.com/interpretml/interpret"}),"Microsoft Interpret")," or\n",Object(n.mdx)("a",s({parentName:"li"},{href:"https://github.com/IBM/AIX360"}),"IBM AI Explainability 360")," but also release\ndedicated services such as\n",Object(n.mdx)("a",s({parentName:"li"},{href:"https://cloud.google.com/explainable-ai"}),"Google Explainable AI")),Object(n.mdx)("li",{parentName:"ul"},"Major data science players, software vendors and services providers alike,\nmakes it a key part of their offering such as\n",Object(n.mdx)("a",s({parentName:"li"},{href:"https://blog.dataiku.com/announcing-dataiku-7-now-with-deeper-collaboration-and-more-granular-explainability"}),"Dataiku"),",\n",Object(n.mdx)("a",s({parentName:"li"},{href:"https://www.accenture.com/us-en/insights/technology/explainable-ai-human-machine"}),"Accenture"),"\nor ",Object(n.mdx)("a",s({parentName:"li"},{href:"https://www.quantmetry.com/les-livres-blancs/"}),"Quantmetry"),","),Object(n.mdx)("li",{parentName:"ul"},"Startups are launched with explainability as a core benefit such as\n",Object(n.mdx)("a",s({parentName:"li"},{href:"https://www.xplik.ai"}),"xplik"),", ",Object(n.mdx)("a",s({parentName:"li"},{href:"https://www.dreamquark.com"}),"DreamQuark")," and of\ncourse, the company I co-created ",Object(n.mdx)("a",s({parentName:"li"},{href:"https://www.craft.ai"}),"craft ai"),"!")),Object(n.mdx)("p",null,"That's not all, XAI is gets discussed by AI ",Object(n.mdx)("em",{parentName:"p"},"influencers"),", MOOCs and conferences\nare created to discuss about it!"),Object(n.mdx)("p",null,"Explainability is important, everyone seems to agree!"),Object(n.mdx)("p",null,"This article aims at exploring ",Object(n.mdx)("strong",{parentName:"p"},"why")," it is important, what impact\nexplainability has when deploying AIs in the ",Object(n.mdx)("em",{parentName:"p"},"real")," world."),Object(n.mdx)("h3",{id:"what-is-an-explanation",style:{position:"relative"}},Object(n.mdx)("a",s({parentName:"h3"},{href:"#what-is-an-explanation","aria-label":"what is an explanation permalink",className:"anchor before"}),Object(n.mdx)("svg",s({parentName:"a"},{"aria-hidden":"true",height:"20",version:"1.1",viewBox:"0 0 16 16",width:"20"}),Object(n.mdx)("path",s({parentName:"svg"},{fillRule:"evenodd",d:"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"})))),"What is an explanation?"),Object(n.mdx)("p",null,"Before talking about Explainable AI, let's talk about explanations! XAI is about\nproviding explanations regarding an AI to its stakeholders, it is therefore\ninteresting to look at how people explain their decisions to each others. To\nhelp design and understand XAI, we can benefit from the learnings of social\nsciences on explanations. That's exactly why,\n",Object(n.mdx)("a",s({parentName:"p"},{href:"https://arxiv.org/abs/1706.07269"}),"Tim Miller")," studied works from various\nbranches of social sciences from philosophy to cognitive sciences and\npsychology."),Object(n.mdx)(o.a,{title:"The three stages of Explainable AI - What is an explanation",src:t.data.slides.publicURL,start:14,end:15,noPagination:!0,noControls:!0,mdxType:"Pdf"}),Object(n.mdx)("p",null,"The first learning extracted from the surveyed body of work is that people seek\nto build a mental model of how stuffs make decisions, how they react to a change\nin context, in order to anticipate them and reason about them. Explanations are\na way to build such ",Object(n.mdx)("em",{parentName:"p"},"models")," much quicker than through observation only."),Object(n.mdx)("p",null,"Because mental models are inherently subjective, good explanations are biased\ntowards the explainee to match their perspective and their preexisting\nknowledge. In the real world examples we describe in the later sections, we\nfound that the work of understanding the point of view of the explainee is a\nmajor part of the design of XAIs."),Object(n.mdx)("p",null,"Another major finding is that good explanations are contrastive. It is not about\nanswering ",Object(n.mdx)("em",{parentName:"p"},'"why has event E occurred?"')," but rather ",Object(n.mdx)("em",{parentName:"p"},'"why has event E occurred\ninstead of event C?"'),". We found out that the capability to generate such\n",Object(n.mdx)("em",{parentName:"p"},"constrative")," or ",Object(n.mdx)("em",{parentName:"p"},"counterfactual")," explanations is quite important in the\ndeployed systems we describe ",Object(n.mdx)("a",s({parentName:"p"},{href:"#stage-2-explainable-decisions"}),"later"),"."),Object(n.mdx)("p",null,"Miller argues that Explainable AI as a field should be considered at the\ncrossroad of Social Science, Human-Computer Interaction and Artificial\nIntelligence. Taking a more practical approach, in this article we will take the\npoint of view of the people and systems interacting with AIs, and study how\nexplainability impacts these interactions in terms of features, acceptance and\ncapacity to be deployed."),Object(n.mdx)("h3",{id:"non-explainable-ai",style:{position:"relative"}},Object(n.mdx)("a",s({parentName:"h3"},{href:"#non-explainable-ai","aria-label":"non explainable ai permalink",className:"anchor before"}),Object(n.mdx)("svg",s({parentName:"a"},{"aria-hidden":"true",height:"20",version:"1.1",viewBox:"0 0 16 16",width:"20"}),Object(n.mdx)("path",s({parentName:"svg"},{fillRule:"evenodd",d:"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"})))),"Non-explainable AI"),Object(n.mdx)(o.a,{title:"The three stages of Explainable AI - Non-explainable AI != Non-detemistic AI",src:t.data.slides.publicURL,start:15,end:16,noPagination:!0,noControls:!0,mdxType:"Pdf"}),Object(n.mdx)("p",null,"Let's do a last detour before actually talking about XAIs, let's ponder what\nmakes an AI non-explainable and also let's talk about the difference between\nexplainability and determinism."),Object(n.mdx)("p",null,"Neural networks (NN) are often considered to be the epitome of black boxes, of\nnon explainable AIs. To understand why you need basic understanding of\n",Object(n.mdx)("a",s({parentName:"p"},{href:"https://www.superdatascience.com/blogs/artificial-neural-networks-how-do-neural-networks-work"}),"how NNs work"),":\neach neuron in the network computes a linear combination of its input variables\nusing learned coefficient and then applies an activation function to the result,\neach layer of neurons works on the previous layer output, up until the actual\ninput variables. The result of the NN from a given input is therefore basically\na linear combination of linear combinations, of ... of the input variables."),Object(n.mdx)("p",null,"This basically means two things:"),Object(n.mdx)("ol",null,Object(n.mdx)("li",{parentName:"ol"},"The NN computation can be written down as a (large) equation and computed by\nhand, there is no magic, just maths, from a given input the output will\nalways be the same, a NN is completely deterministic ;"),Object(n.mdx)("li",{parentName:"ol"},"Unless the NN is really small, it is really difficult to understand how a\nvariation in the input will impact the output: weights interactions are\ndifficult to grasp and dimensional analysis is not possible, as the\ncomputation just manipulate numbers with no regards to their quantity.")),Object(n.mdx)("p",null,"In short, basic maths makes it easy to apply a NN to an input, the nature of the\ncomputation makes the output very difficult to predict its behavior. That what\nmakes NN non-explainable, even the best specialists have a hard time creating\nmental model for a trained neural network."),Object(n.mdx)(o.a,{title:"The three stages of Explainable AI - Adversarial attacks",src:t.data.slides.publicURL,start:16,end:17,noPagination:!0,noControls:!0,mdxType:"Pdf"}),Object(n.mdx)("p",null,"The asymmetry between easy to compute but difficult to understand makes it a\ngood target for ",Object(n.mdx)("em",{parentName:"p"},"adversarial attacks"),". An automated process analyze the NN\nequation to find ",Object(n.mdx)("em",{parentName:"p"},"weak spots")," that couldn't be spotted by the designers of the\nNN. Those weak spots enables the attacking process to create subtle change to\nthe input that are invisible to human experts but confuses the NN. For instance,\none can make a NN powered vision AI mistake a washer for a loudspeaker by\nchanging only a few pixels. In this case the lack of explainability makes it\nimpossible for a human too predict defects in the deployed AI."),Object(n.mdx)("h2",{id:"how-xai-makes-a-difference",style:{position:"relative"}},Object(n.mdx)("a",s({parentName:"h2"},{href:"#how-xai-makes-a-difference","aria-label":"how xai makes a difference permalink",className:"anchor before"}),Object(n.mdx)("svg",s({parentName:"a"},{"aria-hidden":"true",height:"20",version:"1.1",viewBox:"0 0 16 16",width:"20"}),Object(n.mdx)("path",s({parentName:"svg"},{fillRule:"evenodd",d:"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"})))),"How XAI makes a difference"),Object(n.mdx)("p",null,"In order to study the impact explainability makes on AI projects we are\ncategorizing effects in three stages. Higher stages require higher levels of\nexplainability and have more impact on the resulting AIs. We take the point of\nview of the industrial world, and look at how explainability can make a\ndifference in the deployment and application of AI."),Object(n.mdx)("p",null,"This work is based on the experience we gathered working and discussing with our\ncustomers, partners and community, as a provider of machine learning solutions.\nExamples are focused on systems based on Machine Learning but the proposed three\nstages are relevant to any kind of AI."),Object(n.mdx)("h3",{id:"stage-1-explainable-building-process",style:{position:"relative"}},Object(n.mdx)("a",s({parentName:"h3"},{href:"#stage-1-explainable-building-process","aria-label":"stage 1 explainable building process permalink",className:"anchor before"}),Object(n.mdx)("svg",s({parentName:"a"},{"aria-hidden":"true",height:"20",version:"1.1",viewBox:"0 0 16 16",width:"20"}),Object(n.mdx)("path",s({parentName:"svg"},{fillRule:"evenodd",d:"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"})))),"Stage 1: Explainable building process"),Object(n.mdx)(o.a,{title:"The three stages of Explainable AI - Stage 1: Explainable building process",src:t.data.slides.publicURL,start:18,end:19,noPagination:!0,noControls:!0,mdxType:"Pdf"}),Object(n.mdx)("p",null,"In any organisation, just like any IT project, a project leveraging AI aims to\nhave an impact on the daily job of some people. Its goal might even directly be\nto automate part of worker's job or to help them deliver value they could not\nbefore. Especially when AI is involved, affected users can be wary of the new\nsystem. In particular they may feel threatened by the automation of some of\ntheir tasks, or may not believe that a simple ",Object(n.mdx)("em",{parentName:"p"},"computer program")," can execute\ncomplex tasks correctly. A recommandable method to address those concerns is to\ninvolve them in the building of the AI. This is where explainability plays a big\nrole."),Object(n.mdx)(o.a,{title:"The three stages of Explainable AI - Stage 1: Explainable building process, some tools",src:t.data.slides.publicURL,start:19,end:20,noPagination:!0,noControls:!0,mdxType:"Pdf"}),Object(n.mdx)("p",null,"In this context, traditional quality metrics such as confusion matrices, r2,\nRMSE, MAE, etc. are not sufficient to get the future AI user's trust, since they\nwant to know more about the ",Object(n.mdx)("em",{parentName:"p"},"why")," than about the raw results. Visualization is\nthe first go-to technique. Simply plotting the output against context variables\nis a good way to get a ",Object(n.mdx)("em",{parentName:"p"},"feel")," for how an AI performs over the target domain when\ndimensionality is low. Interactive simulations can help explore the domain to\nexperience how the AI will react. Beyond these techniques which are applicable\nto any ",Object(n.mdx)("em",{parentName:"p"},"black box")," computations, more advanced techniques open the hood and make\nthe structure of the AI itself inspectable."),Object(n.mdx)("p",null,"In the following sections describing the subsequent stages, we will talk about\ntechniques able to work while the AIs are ",Object(n.mdx)("em",{parentName:"p"},"live"),", processing production data, at\nproduction speed. These techniques are also well suited for stage 1, where the\ninspection is offline, with less data and runtime constraints."),Object(n.mdx)(o.a,{title:"The three stages of Explainable AI - Stage 1: Explainable building process, examples",src:t.data.slides.publicURL,start:20,end:22,noPagination:!0,noControls:!0,mdxType:"Pdf"}),Object(n.mdx)("p",null,"Debugging tools that were initially designed for data scientists can also be\nleveraged for other stakeholders. AIs powered by neural networks can be\ninspected by visualizing how intermediate layers ",Object(n.mdx)("em",{parentName:"p"},"react")," to different input,\n",Object(n.mdx)("a",s({parentName:"p"},{href:"https://playground.tensorflow.org"}),"Tensorflow Playground")," or\n",Object(n.mdx)("a",s({parentName:"p"},{href:"https://cs.stanford.edu/people/karpathy/convnetjs/docs.html"}),"ConvnetJS")," are\ngood examples of this approach. On images, the computation of\n",Object(n.mdx)("a",s({parentName:"p"},{href:"https://arxiv.org/abs/1312.6034"}),Object(n.mdx)("em",{parentName:"a"},"saliency maps"))," can also help to convey which\nparts of the image are considered by the network to make its prediction. This\ntechnique led to the identification of the infamous\n",Object(n.mdx)("a",s({parentName:"p"},{href:"https://arxiv.org/abs/1602.04938"}),Object(n.mdx)("em",{parentName:"a"},"husky vs wolf"))," issue in which a wolf is\nprimarily identified by the presence of snow in the picture. Tools like\n",Object(n.mdx)("a",s({parentName:"p"},{href:"https://seq2seq-vis.io"}),"Seq2Seq Vis")," bring the same kind of debugging\ncapabilities to natural language focused neural networks. This shows that even\nneural networks, which are considered black boxes, can be at least partly\nexplained offline to the non-technical AI project stakeholder by using the right\ntools."),Object(n.mdx)("p",null,"While the initial goal of explaining why the AI works the way it does is to ease\nits adoption, explainability also increases the involvement of potential users\nby letting them achieve a deeper understanding. As a result they can assist in\nits development, ensuring that the AI solves an actual problem, and provide\nvaluable feedback on specific behaviors of the AI: instead of providing\nknowledge upfront, it is always easier to ",Object(n.mdx)("em",{parentName:"p"},"react")," to what you see the AI doing\nand why it does it. In many cases, domain experts can easily help if they have\nan understanding of why the AI makes decisions: sensors having an undocumented\nvalidity domain, well-known contexts leading to corrupted data, spurious\ncorrelations because of a missing data sources, etc."),Object(n.mdx)("p",null,"The first stage of explainability is about helping create a multi disciplinary\nteam of experts in their respective fields who understand the AI they are\nbuilding. Offline explainability techniques are key to the acceptance of the\nfuture AI and create opportunities to build a better system."),Object(n.mdx)("h3",{id:"stage-2-explainable-decisions",style:{position:"relative"}},Object(n.mdx)("a",s({parentName:"h3"},{href:"#stage-2-explainable-decisions","aria-label":"stage 2 explainable decisions permalink",className:"anchor before"}),Object(n.mdx)("svg",s({parentName:"a"},{"aria-hidden":"true",height:"20",version:"1.1",viewBox:"0 0 16 16",width:"20"}),Object(n.mdx)("path",s({parentName:"svg"},{fillRule:"evenodd",d:"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"})))),"Stage 2: Explainable decisions"),Object(n.mdx)(o.a,{title:"The three stages of Explainable AI - Stage 2: Explainable decisions",src:t.data.slides.publicURL,start:22,end:23,noPagination:!0,noControls:!0,mdxType:"Pdf"}),Object(n.mdx)("p",null,"Trust in a system is key, especially in an enterprise tool that has an impact on\nday to day business. Trust makes the difference between a system that is ",Object(n.mdx)("em",{parentName:"p"},'"micro\nmanaged"')," by its users or supervisors, and a system that can enjoy a larger\nautonomy. The more management a system needs, the more manpower it requires and\ntherefore the less value it has."),Object(n.mdx)("p",null,"Trust is built when a system is not surprising, when it behaves according to our\nmental model. A system whose limits are understood by its users is arguably more\nvaluable than a more accurate system whose results are considered unreliable. As\ndiscussed ",Object(n.mdx)("a",s({parentName:"p"},{href:"#what-is-an-explanation"}),"above"),", explanations are a good way to\naccelerate the construction of this mental model. That is where the capacity to\nexplain the AIs' decisions has an impact. That is the second stage of\nexplainable AI."),Object(n.mdx)("p",null,"Stage 1 explainability does not have the same impact: most users or supervisors\nof AIs did not have the chance to participate in their inception, and in more\nand more cases, AIs can evolve over time. Furthermore, the ability to access\nexplanations of past AI decisions can help pinpoint root causes and generally\nprovide traceability."),Object(n.mdx)(o.a,{title:"The three stages of Explainable AI - Stage 2: Explainable decisions, some tools",src:t.data.slides.publicURL,start:23,end:24,noPagination:!0,noControls:!0,mdxType:"Pdf"}),Object(n.mdx)("p",null,"The ability to provide explanations to any AI decision is an active field of\ninnovation with methods such as\n",Object(n.mdx)("a",s({parentName:"p"},{href:"http://blog.datadive.net/random-forest-interpretation-with-scikit-learn"}),"TreeInterpreter"),"\n",Object(n.mdx)("a",s({parentName:"p"},{href:"https://arxiv.org/abs/1606.05386"}),"LIME")," or\n",Object(n.mdx)("a",s({parentName:"p"},{href:"https://arxiv.org/abs/1705.07874"}),"SHAP"),". Given a predictive model and a\nprediction, these methods aim at providing a local explanation for the\nprediction. This explanation takes the form of linear factors that can be\napplied to input features to reach the predicted results, thus giving an idea of\nthe local feature importance and behavior of the model. The computed feature\nfactors can also be used to generated counterfactual examples and give an idea\nof the trend of the predicted value given changes in the input features."),Object(n.mdx)("p",null,"An interesting property of this class of algorithm is that they can work using a\nfeature set that is different from the actual feature set used by the model. It\nis therefore possible to adapt the explanation by making it more comprehensible\nto the explainee, independently from the features that yield the best\npredictions. This additional feature engineering step is not without risk,\n",Object(n.mdx)("a",s({parentName:"p"},{href:"https://hal.sorbonne-universite.fr/hal-02184519"}),"Christophe Denis and Frank Varenne"),"\nargue, it can be used to convince explainees to blindly trust said AI, by\npresenting a deceptive approximation instead of bringing more transparency."),Object(n.mdx)(o.a,{title:"The three stages of Explainable AI - Stage 2: Explainable decisions, SHAP",src:t.data.slides.publicURL,start:24,end:26,noPagination:!0,noControls:!0,mdxType:"Pdf"}),Object(n.mdx)("p",null,"A good example of SHAP usage can be found in the banking fraud detection\nsolution provided by\n",Object(n.mdx)("a",s({parentName:"p"},{href:"https://www.craft.ai/blog/ai-night-2019-explainable-ai-workshop"}),"Bleckwen"),"\ncompany. One key part of the solution is a predictive model, trained on labelled\ndatasets containing fraudulent and non-fraudulent transactions. This model\ncomputes a score for each transactions. Transactions having a score above a\ncertain threshold are reviewed by a human expert to confirm their fraudulent\nnature. One of their customers' requirements is to get explanations for every\nscore. They chose to use non-explainable gradient boosting techniques for the\nmodel on a range of complex features. The local explanation is computed by SHAP\non a range of features they designed with their end users to make them\ncompletely understandable to them."),Object(n.mdx)(o.a,{title:"The three stages of Explainable AI - Stage 2: Explainable decisions, Dalkia",src:t.data.slides.publicURL,start:26,end:27,noPagination:!0,noControls:!0,mdxType:"Pdf"}),Object(n.mdx)("p",null,"Another example of stage 2 explainable AI is how\n",Object(n.mdx)("a",s({parentName:"p"},{href:"https://www.craft.ai/blog/client-ia-dalkia"}),"Dalkia uses machine learning as a part of their energy management dashboard"),".\nHere, decision trees are used to predict an energy diagnosis based on labelled\ndata streams. Predictions are used as diagnosis recommendations in the energy\nmanagers' dashboard, and explanations are extracted from the decision tree as a\nset of rules that were applied. What's really interesting in this example is\nthat without explanations alongside the recommendations this AI would not have\nany value. At its core, the goal of the system is to help energy managers handle\nmore data points. Without an explanation, when provided with a prediction,\nenergy managers would need to investigate the raw data in order to confirm or\ncontradict it. They would end up doing the same amount of work as without\nexplanations. When an explanation is provided, this counter investigation is\nonly needed when the energy manager disagrees with it. Here, explanations are\nneeded for the business value of the AI."),Object(n.mdx)("h3",{id:"stage-3-explainable-decision-process",style:{position:"relative"}},Object(n.mdx)("a",s({parentName:"h3"},{href:"#stage-3-explainable-decision-process","aria-label":"stage 3 explainable decision process permalink",className:"anchor before"}),Object(n.mdx)("svg",s({parentName:"a"},{"aria-hidden":"true",height:"20",version:"1.1",viewBox:"0 0 16 16",width:"20"}),Object(n.mdx)("path",s({parentName:"svg"},{fillRule:"evenodd",d:"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"})))),"Stage 3: Explainable decision process"),Object(n.mdx)(o.a,{title:"The three stages of Explainable AI - Stage 3: Explainable decision process",src:t.data.slides.publicURL,start:27,end:28,noPagination:!0,noControls:!0,mdxType:"Pdf"}),Object(n.mdx)("p",null,"Stages 1 and 2 are about helping humans create a mental model of how AIs\noperate. This enables humans to ",Object(n.mdx)("em",{parentName:"p"},'"reason"')," about the way AIs work critically,\nand decide when to trust them and accept their outputs, predictions or\nrecommendations. To scale this up to many AIs and over time, you need to define\nbusiness logic that will apply the same ",Object(n.mdx)("em",{parentName:"p"},'"reasoning"')," automatically. Stage 3 is\nabout enabling interoperability between AIs and other pieces of software,\nespecially software that uses business logic."),Object(n.mdx)("p",null,"When discussing AI, and especially models generated through machine learning, we\noften talk about the underlying concepts they capture, for example convolutional\nneural networks are able to recognize visual patterns and build upon these lower\nlevel ",Object(n.mdx)("em",{parentName:"p"},'"concepts"')," in their predictions. AIs that can explain those lower level\nbuilding blocks, make them inspectable to business logic, reach stage 3. Such\nAIs ultimately act as a knowledge base of the behavior they model."),Object(n.mdx)("p",null,"Stage 3 explainability makes a difference especially when a lot of instances of\nevolving AIs need to be supervised by business logic, for example in a context\nof continuous certifiability or collaborative automation between machine\nlearning based AIs and business rules."),Object(n.mdx)(o.a,{title:"The three stages of Explainable AI - Stage 3: Explainable decision process, some tools",src:t.data.slides.publicURL,start:28,end:29,noPagination:!0,noControls:!0,mdxType:"Pdf"}),Object(n.mdx)("p",null,"This level of explainability requires fully explainable AI. Machine learning\ntechniques such as linear regressions or decision tree learning can reach such\nlevels. Another approach is to approximate a more ",Object(n.mdx)("em",{parentName:"p"},'"black box"')," model with a\nmore explainable model, for example ",Object(n.mdx)("a",s({parentName:"p"},{href:"https://arxiv.org/abs/0811.1679"}),"RuleFit"),"\nis able to learn a minimal ensemble of rules from a tree ensemble method such as\nRandom Forest."),Object(n.mdx)(o.a,{title:"The three stages of Explainable AI - Stage 3: Explainable decision process, exemple",src:t.data.slides.publicURL,start:29,end:30,noPagination:!0,noControls:!0,mdxType:"Pdf"}),Object(n.mdx)("p",null,"An interesting example of level 3 explainability is\n",Object(n.mdx)("a",s({parentName:"p"},{href:"https://www.craft.ai/blog/how-total-direct-energie-applies-explainable-ai-to-its-virtual-assistant"}),"Total Direct Energie's energy coaching feature"),"\nthat is part of their customer-facing mobile application. It generates\npersonalized messages for each customer. At its core, the system is made of a\nmachine learning-based energy consumption predictive model, and a business\nexpertise-based message generation and selection module. The predictive model is\nmade of individual regression trees, each updated continuously from the data of\na single household. The message generation module is generic for all users, and\nuses the model's explanations and predictions as input data to select and\npersonalize each message. So the predictive models provide an understanding of\nthe household's energy consumption behavior, which is automatically processed to\ngenerate personalized messages."),Object(n.mdx)("p",null,"When presented with a visual explanation of a decision process, people tend to\nnavigate through its structure to understand the process. Stage 3 is about\nletting software programs, other AIs, do the same thing, thus unlocking a wealth\nof additional use cases."),Object(n.mdx)("h2",{id:"challenges",style:{position:"relative"}},Object(n.mdx)("a",s({parentName:"h2"},{href:"#challenges","aria-label":"challenges permalink",className:"anchor before"}),Object(n.mdx)("svg",s({parentName:"a"},{"aria-hidden":"true",height:"20",version:"1.1",viewBox:"0 0 16 16",width:"20"}),Object(n.mdx)("path",s({parentName:"svg"},{fillRule:"evenodd",d:"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"})))),"Challenges"),Object(n.mdx)("p",null,"While there are already deployed AIs covering these three stages, there are\nstill challenges ahead before explainable AI can be generalized."),Object(n.mdx)("h3",{id:"evaluating-explanations",style:{position:"relative"}},Object(n.mdx)("a",s({parentName:"h3"},{href:"#evaluating-explanations","aria-label":"evaluating explanations permalink",className:"anchor before"}),Object(n.mdx)("svg",s({parentName:"a"},{"aria-hidden":"true",height:"20",version:"1.1",viewBox:"0 0 16 16",width:"20"}),Object(n.mdx)("path",s({parentName:"svg"},{fillRule:"evenodd",d:"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"})))),"Evaluating explanations"),Object(n.mdx)(o.a,{title:"The three stages of Explainable AI - How can we evaluate explainability?",src:t.data.slides.publicURL,start:30,end:31,noPagination:!0,noControls:!0,mdxType:"Pdf"}),Object(n.mdx)("p",null,"In the previous sections we discussed how certain techniques bring more or less\nexplainability, however we did not discuss how we can make such an assessment."),Object(n.mdx)("p",null,"Ad-hoc experiments or KPI can be used. For example the D-Edge company, which\nprovides pricing recommendations to hotel managers among other services,\nmeasures whether explained recommendations are accepted. Every recommendation is\naccompanied by a natural language explanation. Managers can accept and apply the\nrecommendation to their pricing or discard it. As presented during a\n",Object(n.mdx)("a",s({parentName:"p"},{href:"https://www.craft.ai/blog/ai-night-2019-explainable-ai-workshop"}),"round table focused on XAI"),",\nthey consider the proportion of accepted recommendations as a proxy measure for\nthe quality of their explanation. We believe that this makes sense, as hotel\nmanagers need to be convinced to make such an impactful change to their\nbusiness."),Object(n.mdx)("p",null,"In the general case, other proxy measures can be used, such as the number of\nrules, nodes or input variables considered in an explanation or explainable\nmodel. However these lack generality: how can the explainability of a linear\nregression and of a regression tree be compared? They also lack an experimental,\nmeasurable ground truth: for example we do not know if humans find that the\nexplainability provided by LIME grows exponentially or linearly with the number\nof features involved. Furthermore, as discussed\n",Object(n.mdx)("a",s({parentName:"p"},{href:"#what-is-an-explanation"}),"above"),", what constitutes a good or a bad explanation\ndepends on the recipient of the explanation and their own cognitive biases. This\nposes an additional challenge to this evaluation. There is a lack of a systemic\nframework or objective criteria to evaluate the explanations provided by AIs. A\nchallenge ",Object(n.mdx)("a",s({parentName:"p"},{href:"https://arxiv.org/abs/1708.01870"}),"identified by Adrian Weller"),"."),Object(n.mdx)("h3",{id:"improving-the-performances-of-xai",style:{position:"relative"}},Object(n.mdx)("a",s({parentName:"h3"},{href:"#improving-the-performances-of-xai","aria-label":"improving the performances of xai permalink",className:"anchor before"}),Object(n.mdx)("svg",s({parentName:"a"},{"aria-hidden":"true",height:"20",version:"1.1",viewBox:"0 0 16 16",width:"20"}),Object(n.mdx)("path",s({parentName:"svg"},{fillRule:"evenodd",d:"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"})))),"Improving the performances of XAI"),Object(n.mdx)(o.a,{title:"The three stages of Explainable AI - How can we evaluate explainability?",src:t.data.slides.publicURL,start:31,end:32,noPagination:!0,noControls:!0,mdxType:"Pdf"}),Object(n.mdx)("p",null,"The AI community generally considers that the more explainability you gain, the\nless predictive performance you can achieve, especially in Machine Learning.\nOvercoming this is a primary goal of the XAI field, and in particular it is the\nmain goal of the DARPA XAI program."),Object(n.mdx)("p",null,"Several opportunities have been identified to achieve this objective, the most\npromising ones being to create ",Object(n.mdx)("strong",{parentName:"p"},"hybrid AIs")," combining different approaches."),Object(n.mdx)("p",null,"One idea is to ",Object(n.mdx)("em",{parentName:"p"},"push")," high-performance but unexplainable algorithms to the\nedges, around an explainable core. For example a deep neural networks would\nidentify low level details like whiskers and pointy ears, while decision trees\nor bayesian models would associate the presence of both whiskers and pointy ears\nto a cat in an explainable fashion."),Object(n.mdx)("p",null,"Another idea is to adapt Machine Learning algorithms to work from existing\nexpert-built symbolic representations of physical models to leverage existing\nknowledge, instead of having to relearn and embed it. This field is relatively\nnew, and comes as a stark departure from the deep learning trend of the past few\nyears. This is at the heart of\n",Object(n.mdx)("a",s({parentName:"p"},{href:"https://www.irt-systemx.fr/systemx-lance-le-programme-de-recherche-intelligence-artificielle-et-ingenierie-augmentee-ia2/"}),"IRT SystemX IA2 program"),"."),Object(n.mdx)("h2",{id:"conclusion",style:{position:"relative"}},Object(n.mdx)("a",s({parentName:"h2"},{href:"#conclusion","aria-label":"conclusion permalink",className:"anchor before"}),Object(n.mdx)("svg",s({parentName:"a"},{"aria-hidden":"true",height:"20",version:"1.1",viewBox:"0 0 16 16",width:"20"}),Object(n.mdx)("path",s({parentName:"svg"},{fillRule:"evenodd",d:"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"})))),"Conclusion"),Object(n.mdx)(o.a,{title:"The three stages of Explainable AI - #rocket",src:t.data.slides.publicURL,start:32,end:33,noPagination:!0,noControls:!0,mdxType:"Pdf"}),Object(n.mdx)("p",null,"We structured in three stages the impact that explainability has on AIs deployed\nin the ",Object(n.mdx)("em",{parentName:"p"},'"real world"'),". Those 3 stages provide a simple framework to quickly\nidentify the need for explainability in a AI powered project."),Object(n.mdx)("p",null,Object(n.mdx)("strong",{parentName:"p"},"Stage 1")," is about leveraging explainability to improve the adoption and\nperformance of AIs."),Object(n.mdx)("p",null,Object(n.mdx)("strong",{parentName:"p"},"Stage 2")," is about explaining every AI decisions to build trust with their\nusers and supervisors."),Object(n.mdx)("p",null,Object(n.mdx)("strong",{parentName:"p"},"Stage 3")," is about enabling the interoperability of AIs with each other and\nother software, thus unlocking new and richer use cases."),Object(n.mdx)(o.a,{title:"The three stages of Explainable AI - Takeaway",src:t.data.slides.publicURL,start:33,end:34,noPagination:!0,noControls:!0,mdxType:"Pdf"}),Object(n.mdx)("p",null,"Because we focused on what explainability enables in AI, we did not discuss\nregulation. However it is important to note that initiatives such as the\nEuropean GDPR pave the way for a\n",Object(n.mdx)("a",s({parentName:"p"},{href:"https://iapp.org/news/a/is-there-a-right-to-explanation-for-machine-learning-in-the-gdpr"}),Object(n.mdx)("em",{parentName:"a"},'"right to explanation"')),"\nwhich will require, at least in some cases, a stage 2 requirement. We strongly\nbelieve that stage 2 explainability is a key to actually operationalize\nenterprise AI because it not only offers stronger guarantees in terms of data\ngovernance, but also facilitates involvement and support from users and domain\nexperts impacted by such AI."),Object(n.mdx)("p",null,Object(n.mdx)("strong",{parentName:"p"},"Far from being just a constraint on AI design, explainability is actually an\nopportunity to develop AIs that actually deliver value to Humans.")),Object(n.mdx)(o.a,{title:"The three stages of Explainable AI - Beyond",src:t.data.slides.publicURL,start:34,end:35,noPagination:!0,noControls:!0,mdxType:"Pdf"}),Object(n.mdx)("hr",null),Object(n.mdx)("blockquote",null,Object(n.mdx)("p",{parentName:"blockquote"},'Paper, as published in the proceedings of the "Humains et IA (HIA)" workshop\nof the\n',Object(n.mdx)("a",s({parentName:"p"},{href:"https://egc2020.sciencesconf.org"}),"20th Extraction et Gestion des Connaissances (EGC) conference in 2020"),".")),Object(n.mdx)(o.a,{title:"The three stages of Explainable AI: How explainability facilitates real world deployment of AI",href:"https://github.com/craft-ai/sci/tree/master/publications/2020-the-three-stages-of-xai",src:t.data.paper.publicURL,mdxType:"Pdf"}),Object(n.mdx)("h2",{id:"history",style:{position:"relative"}},Object(n.mdx)("a",s({parentName:"h2"},{href:"#history","aria-label":"history permalink",className:"anchor before"}),Object(n.mdx)("svg",s({parentName:"a"},{"aria-hidden":"true",height:"20",version:"1.1",viewBox:"0 0 16 16",width:"20"}),Object(n.mdx)("path",s({parentName:"svg"},{fillRule:"evenodd",d:"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"})))),"History"),Object(n.mdx)("ul",null,Object(n.mdx)("li",{parentName:"ul"},Object(n.mdx)("strong",{parentName:"li"},"2019/03/13")," -\n",Object(n.mdx)("a",s({parentName:"li"},{href:"https://nuit-blanche.blogspot.com/2019/03/ce-soir-paris-machine-learning-5-season.html"}),"Paris Machine Learning Meetup #5, season 6"),"."),Object(n.mdx)("li",{parentName:"ul"},Object(n.mdx)("strong",{parentName:"li"},"2019/04/18")," -\n",Object(n.mdx)("a",s({parentName:"li"},{href:"https://www.craft.ai/blog/ai-night-2019-explainable-ai-workshop"}),"AI Night ",Object(n.mdx)("em",{parentName:"a"},"explainability")," roundtable"),"."),Object(n.mdx)("li",{parentName:"ul"},Object(n.mdx)("strong",{parentName:"li"},"2019/05/24")," & ",Object(n.mdx)("strong",{parentName:"li"},"2019/07/18")," -\n",Object(n.mdx)("a",s({parentName:"li"},{href:"https://sc21.fr/total-retour-sur-la-session-aiforleaders-du-24-mai/"}),"#AIforLeaders workshops at Total"),"."),Object(n.mdx)("li",{parentName:"ul"},Object(n.mdx)("strong",{parentName:"li"},"2019/10/07")," -\n",Object(n.mdx)("a",s({parentName:"li"},{href:"https://ax.polytechnique.org/newsletter/preview/index/id/503/noLayout/1"}),"X-IA #6 ",Object(n.mdx)("em",{parentName:"a"},"Intelligibilité des modèles")," Meetup")),Object(n.mdx)("li",{parentName:"ul"},Object(n.mdx)("strong",{parentName:"li"},"2019/10/17")," -\n",Object(n.mdx)("a",s({parentName:"li"},{href:"https://www.association-aristote.fr/lia-est-elle-explicable/"}),"Séminaire Aristote, l’IA est-elle explicable ? Un coup d'oeil furtif dans la boite noire des algorithmes de l'IA"),"."),Object(n.mdx)("li",{parentName:"ul"},Object(n.mdx)("strong",{parentName:"li"},"2020/01/28")," -\n",Object(n.mdx)("a",s({parentName:"li"},{href:"http://headwork.gforge.inria.fr/HIA2020/index"}),"Humains et IA (HIA) workshop"),"\nof the 20th Extraction et Gestion des Connaissances (EGC) conference.")))}p.isMDXComponent=!0}}]);
//# sourceMappingURL=component---src-pages-articles-2020-04-22-the-three-stages-of-explainable-ai-index-mdx-86dcf7b7a4cd776d1ec9.js.map