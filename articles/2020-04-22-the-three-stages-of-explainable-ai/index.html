<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="x-ua-compatible" content="ie=edge"/><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/><style data-href="/styles.6c3894427c5e92cff8ca.css">svg:not(:root).svg-inline--fa{overflow:visible}.svg-inline--fa{display:inline-block;font-size:inherit;height:1em;overflow:visible;vertical-align:-.125em}.svg-inline--fa.fa-lg{vertical-align:-.225em}.svg-inline--fa.fa-w-1{width:.0625em}.svg-inline--fa.fa-w-2{width:.125em}.svg-inline--fa.fa-w-3{width:.1875em}.svg-inline--fa.fa-w-4{width:.25em}.svg-inline--fa.fa-w-5{width:.3125em}.svg-inline--fa.fa-w-6{width:.375em}.svg-inline--fa.fa-w-7{width:.4375em}.svg-inline--fa.fa-w-8{width:.5em}.svg-inline--fa.fa-w-9{width:.5625em}.svg-inline--fa.fa-w-10{width:.625em}.svg-inline--fa.fa-w-11{width:.6875em}.svg-inline--fa.fa-w-12{width:.75em}.svg-inline--fa.fa-w-13{width:.8125em}.svg-inline--fa.fa-w-14{width:.875em}.svg-inline--fa.fa-w-15{width:.9375em}.svg-inline--fa.fa-w-16{width:1em}.svg-inline--fa.fa-w-17{width:1.0625em}.svg-inline--fa.fa-w-18{width:1.125em}.svg-inline--fa.fa-w-19{width:1.1875em}.svg-inline--fa.fa-w-20{width:1.25em}.svg-inline--fa.fa-pull-left{margin-right:.3em;width:auto}.svg-inline--fa.fa-pull-right{margin-left:.3em;width:auto}.svg-inline--fa.fa-border{height:1.5em}.svg-inline--fa.fa-li{width:2em}.svg-inline--fa.fa-fw{width:1.25em}.fa-layers svg.svg-inline--fa{bottom:0;left:0;margin:auto;position:absolute;right:0;top:0}.fa-layers{display:inline-block;height:1em;position:relative;text-align:center;vertical-align:-.125em;width:1em}.fa-layers svg.svg-inline--fa{-webkit-transform-origin:center center;transform-origin:center center}.fa-layers-counter,.fa-layers-text{display:inline-block;position:absolute;text-align:center}.fa-layers-text{left:50%;top:50%;-webkit-transform:translate(-50%,-50%);transform:translate(-50%,-50%);-webkit-transform-origin:center center;transform-origin:center center}.fa-layers-counter{background-color:#ff253a;border-radius:1em;box-sizing:border-box;color:#fff;height:1.5em;line-height:1;max-width:5em;min-width:1.5em;overflow:hidden;padding:.25em;right:0;text-overflow:ellipsis;top:0;-webkit-transform:scale(.25);transform:scale(.25);-webkit-transform-origin:top right;transform-origin:top right}.fa-layers-bottom-right{bottom:0;right:0;top:auto;-webkit-transform:scale(.25);transform:scale(.25);-webkit-transform-origin:bottom right;transform-origin:bottom right}.fa-layers-bottom-left{bottom:0;left:0;right:auto;top:auto;-webkit-transform:scale(.25);transform:scale(.25);-webkit-transform-origin:bottom left;transform-origin:bottom left}.fa-layers-top-right{right:0;top:0;-webkit-transform:scale(.25);transform:scale(.25);-webkit-transform-origin:top right;transform-origin:top right}.fa-layers-top-left{left:0;right:auto;top:0;-webkit-transform:scale(.25);transform:scale(.25);-webkit-transform-origin:top left;transform-origin:top left}.fa-lg{font-size:1.33333em;line-height:.75em;vertical-align:-.0667em}.fa-xs{font-size:.75em}.fa-sm{font-size:.875em}.fa-1x{font-size:1em}.fa-2x{font-size:2em}.fa-3x{font-size:3em}.fa-4x{font-size:4em}.fa-5x{font-size:5em}.fa-6x{font-size:6em}.fa-7x{font-size:7em}.fa-8x{font-size:8em}.fa-9x{font-size:9em}.fa-10x{font-size:10em}.fa-fw{text-align:center;width:1.25em}.fa-ul{list-style-type:none;margin-left:2.5em;padding-left:0}.fa-ul>li{position:relative}.fa-li{left:-2em;position:absolute;text-align:center;width:2em;line-height:inherit}.fa-border{border:.08em solid #eee;border-radius:.1em;padding:.2em .25em .15em}.fa-pull-left{float:left}.fa-pull-right{float:right}.fa.fa-pull-left,.fab.fa-pull-left,.fal.fa-pull-left,.far.fa-pull-left,.fas.fa-pull-left{margin-right:.3em}.fa.fa-pull-right,.fab.fa-pull-right,.fal.fa-pull-right,.far.fa-pull-right,.fas.fa-pull-right{margin-left:.3em}.fa-spin{-webkit-animation:fa-spin 2s linear infinite;animation:fa-spin 2s linear infinite}.fa-pulse{-webkit-animation:fa-spin 1s steps(8) infinite;animation:fa-spin 1s steps(8) infinite}@-webkit-keyframes fa-spin{0%{-webkit-transform:rotate(0deg);transform:rotate(0deg)}to{-webkit-transform:rotate(1turn);transform:rotate(1turn)}}@keyframes fa-spin{0%{-webkit-transform:rotate(0deg);transform:rotate(0deg)}to{-webkit-transform:rotate(1turn);transform:rotate(1turn)}}.fa-rotate-90{-ms-filter:"progid:DXImageTransform.Microsoft.BasicImage(rotation=1)";-webkit-transform:rotate(90deg);transform:rotate(90deg)}.fa-rotate-180{-ms-filter:"progid:DXImageTransform.Microsoft.BasicImage(rotation=2)";-webkit-transform:rotate(180deg);transform:rotate(180deg)}.fa-rotate-270{-ms-filter:"progid:DXImageTransform.Microsoft.BasicImage(rotation=3)";-webkit-transform:rotate(270deg);transform:rotate(270deg)}.fa-flip-horizontal{-ms-filter:"progid:DXImageTransform.Microsoft.BasicImage(rotation=0, mirror=1)";-webkit-transform:scaleX(-1);transform:scaleX(-1)}.fa-flip-vertical{-webkit-transform:scaleY(-1);transform:scaleY(-1)}.fa-flip-both,.fa-flip-horizontal.fa-flip-vertical,.fa-flip-vertical{-ms-filter:"progid:DXImageTransform.Microsoft.BasicImage(rotation=2, mirror=1)"}.fa-flip-both,.fa-flip-horizontal.fa-flip-vertical{-webkit-transform:scale(-1);transform:scale(-1)}:root .fa-flip-both,:root .fa-flip-horizontal,:root .fa-flip-vertical,:root .fa-rotate-90,:root .fa-rotate-180,:root .fa-rotate-270{-webkit-filter:none;filter:none}.fa-stack{display:inline-block;height:2em;position:relative;width:2.5em}.fa-stack-1x,.fa-stack-2x{bottom:0;left:0;margin:auto;position:absolute;right:0;top:0}.svg-inline--fa.fa-stack-1x{height:1em;width:1.25em}.svg-inline--fa.fa-stack-2x{height:2em;width:2.5em}.fa-inverse{color:#fff}.sr-only{border:0;clip:rect(0,0,0,0);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px}.sr-only-focusable:active,.sr-only-focusable:focus{clip:auto;height:auto;margin:0;overflow:visible;position:static;width:auto}.svg-inline--fa .fa-primary{fill:var(--fa-primary-color,currentColor);opacity:1;opacity:var(--fa-primary-opacity,1)}.svg-inline--fa .fa-secondary{fill:var(--fa-secondary-color,currentColor)}.svg-inline--fa .fa-secondary,.svg-inline--fa.fa-swap-opacity .fa-primary{opacity:.4;opacity:var(--fa-secondary-opacity,.4)}.svg-inline--fa.fa-swap-opacity .fa-secondary{opacity:1;opacity:var(--fa-primary-opacity,1)}.svg-inline--fa mask .fa-primary,.svg-inline--fa mask .fa-secondary{fill:#000}.fad.fa-inverse{color:#fff}

/*! normalize.css v8.0.1 | MIT License | github.com/necolas/normalize.css */html{line-height:1.15;-webkit-text-size-adjust:100%}body{margin:0}main{display:block}h1{font-size:2em;margin:.67em 0}hr{box-sizing:content-box;height:0;overflow:visible}pre{font-family:monospace,monospace;font-size:1em}a{background-color:transparent}abbr[title]{border-bottom:none;text-decoration:underline;text-decoration:underline dotted}b,strong{font-weight:bolder}code,kbd,samp{font-family:monospace,monospace;font-size:1em}small{font-size:80%}sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}sub{bottom:-.25em}sup{top:-.5em}img{border-style:none}button,input,optgroup,select,textarea{font-family:inherit;font-size:100%;line-height:1.15;margin:0}button,input{overflow:visible}button,select{text-transform:none}[type=button],[type=reset],[type=submit],button{-webkit-appearance:button}[type=button]::-moz-focus-inner,[type=reset]::-moz-focus-inner,[type=submit]::-moz-focus-inner,button::-moz-focus-inner{border-style:none;padding:0}[type=button]:-moz-focusring,[type=reset]:-moz-focusring,[type=submit]:-moz-focusring,button:-moz-focusring{outline:1px dotted ButtonText}fieldset{padding:.35em .75em .625em}legend{box-sizing:border-box;color:inherit;display:table;max-width:100%;padding:0;white-space:normal}progress{vertical-align:baseline}textarea{overflow:auto}[type=checkbox],[type=radio]{box-sizing:border-box;padding:0}[type=number]::-webkit-inner-spin-button,[type=number]::-webkit-outer-spin-button{height:auto}[type=search]{-webkit-appearance:textfield;outline-offset:-2px}[type=search]::-webkit-search-decoration{-webkit-appearance:none}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}details{display:block}summary{display:list-item}template{display:none}[hidden]{display:none}@font-face{font-family:Quicksand;font-style:normal;font-display:swap;font-weight:300;src:local("Quicksand Light "),local("Quicksand-Light"),url(/static/quicksand-latin-300-7555adc38bfc401504f137e1e604ed84.woff2) format("woff2"),url(/static/quicksand-latin-300-9a2540cce836fdfccdf35a7184b24af8.woff) format("woff")}@font-face{font-family:Quicksand;font-style:normal;font-display:swap;font-weight:400;src:local("Quicksand Regular "),local("Quicksand-Regular"),url(/static/quicksand-latin-400-aa06b1e97f0640360f4b7355cdd0e4ab.woff2) format("woff2"),url(/static/quicksand-latin-400-89b7578bfa5a888dae676a596e423cb6.woff) format("woff")}@font-face{font-family:Quicksand;font-style:normal;font-display:swap;font-weight:500;src:local("Quicksand Medium "),local("Quicksand-Medium"),url(/static/quicksand-latin-500-71d7b62210ccadec855179ef008f507f.woff2) format("woff2"),url(/static/quicksand-latin-500-5dacfd42158e8ea0ea6ecd13ed27240e.woff) format("woff")}@font-face{font-family:Quicksand;font-style:normal;font-display:swap;font-weight:600;src:local("Quicksand SemiBold "),local("Quicksand-SemiBold"),url(/static/quicksand-latin-600-63b2ff32faf922e07008a45318493705.woff2) format("woff2"),url(/static/quicksand-latin-600-68229628ae890c4956a344b94454475f.woff) format("woff")}@font-face{font-family:Quicksand;font-style:normal;font-display:swap;font-weight:700;src:local("Quicksand Bold "),local("Quicksand-Bold"),url(/static/quicksand-latin-700-e84fd014ffd68f5d72d276fbe3ced630.woff2) format("woff2"),url(/static/quicksand-latin-700-25e58c7536f16dac306d49aa480abfa1.woff) format("woff")}</style><meta name="generator" content="Gatsby 2.23.9"/><title data-react-helmet="true">The three stages of Explainable AI: How explainability facilitates real world deployment of AI | cloderic.com</title><link data-react-helmet="true" rel="canonical" href="https://www.cloderic.com/articles/2020-04-22-the-three-stages-of-explainable-ai/"/><meta data-react-helmet="true" name="description" content="Clodéric Mars - AI Product Engineer, Tech Leader, Public Speaker, humming from Paris"/><meta data-react-helmet="true" name="author" content="Clodéric Mars"/><meta data-react-helmet="true" name="keywords" content="ai,tech,public speaking"/><meta data-react-helmet="true" name="date" content="2020-04-22T00:00:00.000Z"/><meta data-react-helmet="true" property="og:type" content="article"/><meta data-react-helmet="true" property="og:title" content="The three stages of Explainable AI: How explainability facilitates real world deployment of AI | cloderic.com"/><meta data-react-helmet="true" property="og:site_name" content="cloderic.com"/><meta data-react-helmet="true" property="og:description" content="Clodéric Mars - AI Product Engineer, Tech Leader, Public Speaker, humming from Paris"/><meta data-react-helmet="true" property="og:url" content="https://www.cloderic.com/articles/2020-04-22-the-three-stages-of-explainable-ai/"/><meta data-react-helmet="true" property="og:image" content="https://www.cloderic.com/static/9381c89176963851c7ecba1737a9acb8/a8378/mars.png"/><meta data-react-helmet="true" property="og:image:url" content="https://www.cloderic.com/static/9381c89176963851c7ecba1737a9acb8/a8378/mars.png"/><meta data-react-helmet="true" property="og:image:secure_url" content="https://www.cloderic.com/static/9381c89176963851c7ecba1737a9acb8/a8378/mars.png"/><meta data-react-helmet="true" property="og:image:alt" content="Clodéric Mars - AI Product Engineer, Tech Leader, Public Speaker, humming from Paris"/><meta data-react-helmet="true" property="og:image:width" content="1024"/><meta data-react-helmet="true" property="og:image:height" content="1024"/><meta data-react-helmet="true" property="og:image:type" content="image/png"/><meta data-react-helmet="true" name="twitter:card" content="summary"/><meta data-react-helmet="true" name="twitter:site" content="@cloderic"/><meta data-react-helmet="true" name="twitter:creator" content="@cloderic"/><link rel="icon" href="/icons/icon-48x48.png?v=693f80d46448c9c1e292c54fb7df2d05"/><link rel="manifest" href="/manifest.webmanifest"/><meta name="theme-color" content="#f27830"/><link rel="apple-touch-icon" sizes="48x48" href="/icons/icon-48x48.png?v=693f80d46448c9c1e292c54fb7df2d05"/><link rel="apple-touch-icon" sizes="72x72" href="/icons/icon-72x72.png?v=693f80d46448c9c1e292c54fb7df2d05"/><link rel="apple-touch-icon" sizes="96x96" href="/icons/icon-96x96.png?v=693f80d46448c9c1e292c54fb7df2d05"/><link rel="apple-touch-icon" sizes="144x144" href="/icons/icon-144x144.png?v=693f80d46448c9c1e292c54fb7df2d05"/><link rel="apple-touch-icon" sizes="192x192" href="/icons/icon-192x192.png?v=693f80d46448c9c1e292c54fb7df2d05"/><link rel="apple-touch-icon" sizes="256x256" href="/icons/icon-256x256.png?v=693f80d46448c9c1e292c54fb7df2d05"/><link rel="apple-touch-icon" sizes="384x384" href="/icons/icon-384x384.png?v=693f80d46448c9c1e292c54fb7df2d05"/><link rel="apple-touch-icon" sizes="512x512" href="/icons/icon-512x512.png?v=693f80d46448c9c1e292c54fb7df2d05"/><link rel="sitemap" type="application/xml" href="/sitemap.xml"/><link rel="preconnect dns-prefetch" href="https://www.google-analytics.com"/><link as="script" rel="preload" href="/webpack-runtime-28fbd69803a77b26dfd6.js"/><link as="script" rel="preload" href="/framework-ee697c05f43c773aa564.js"/><link as="script" rel="preload" href="/styles-39e9c908b573a9761a76.js"/><link as="script" rel="preload" href="/a9a7754c-2bf3d99a1f1ef2138d89.js"/><link as="script" rel="preload" href="/cb1608f2-8480d875f23d193a8efc.js"/><link as="script" rel="preload" href="/2b7b2d2a-4f51378a4c7bbaec6330.js"/><link as="script" rel="preload" href="/app-ad690ddceb3d8eaa4fb6.js"/><link as="script" rel="preload" href="/0f1ac474-a5e16e9b990849dc8ee5.js"/><link as="script" rel="preload" href="/46ee4902551c23666229271b7c847a0d0f0f7703-1762214d0be939521d9b.js"/><link as="script" rel="preload" href="/component---src-pages-articles-2020-04-22-the-three-stages-of-explainable-ai-index-mdx-86dcf7b7a4cd776d1ec9.js"/><link as="fetch" rel="preload" href="/page-data/articles/2020-04-22-the-three-stages-of-explainable-ai/page-data.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/app-data.json" crossorigin="anonymous"/></head><body><div id="___gatsby"><div style="outline:none" tabindex="-1" id="gatsby-focus-wrapper"><style data-emotion-css="1jgtt9u">html{font-family:quicksand;font-weight:300;font-size:12pt;color:#FFFFFF;}html *,html *::before,html *::after{box-sizing:border-box;}@media print{html{font-size:9pt;}}html h1{font-weight:400;page-break-after:"";}@media print{html h1{font-weight:500;}}html h2,html h3,html h4,html h5,html h5{font-weight:inherit;page-break-after:"";}@media print{html h2,html h3,html h4,html h5,html h5{font-weight:400;}}html .anchor{fill:#FFFFFF;width:20px;margin-right:5px;margin-left:-25px;}html .anchor svg{visibility:hidden;}html .anchor:hover svg{visibility:visible;}html header{margin:2em 0;}html header h1,html header h2,html header h3,html header h4,html header h5,html header h5,html header p{margin:0.1em 0;}html strong{font-weight:800;}html blockquote{padding-left:0.5em;font-style:italic;border-left:0.2em solid rgba(242,120,48,0.5);margin-block-start:1em;margin-block-end:1em;margin-inline-start:0;margin-inline-end:0;}body{min-height:100vh;background:repeat center / 300px url(/static/d62c0fa0afb58149b0b1c493541056b8/46604/pattern.png),linear-gradient(175deg,#0b2945,#040F19),#040F19;}a,button{color:inherit;cursor:pointer;background:none;border:none;}a:active,button:active,a:focus,button:focus{color:#f48848;outline:none;}a:hover,button:hover{color:#F27830;}a:disabled,button:disabled{color:rgba(255,255,255,0.5);}a:disabled:hover,button:disabled:hover{cursor:not-allowed;}hr{border:0.5px #FFFFFF solid;}</style><main class="css-0 e1ye9trm0"><style data-emotion-css="o65ib4">.css-o65ib4{background:linear-gradient(175deg,#a9bed1,#88A5BF);z-index:10;box-shadow:0 10px 15px -3px rgba(0,0,0,0.2), 0 4px 6px -2px rgba(0,0,0,0.1);}@media print{.css-o65ib4{box-shadow:none;}}.css-o65ib4 p{line-height:1.7;font-size:1.1em;}.css-o65ib4 h1,.css-o65ib4 h2,.css-o65ib4 h3,.css-o65ib4 h4,.css-o65ib4 h5,.css-o65ib4 h6{font-weight:500;}.css-o65ib4 h2,.css-o65ib4 h3,.css-o65ib4 hr,.css-o65ib4 header.articleHeader{margin:2em 0;border:none;padding-bottom:0.5em;border-bottom:1px #F27830 solid;}</style><article class="css-o65ib4 e1g54axm0"><style data-emotion-css="1j83eb7">.css-1j83eb7{margin:0 auto;padding:0 0.5rem;max-width:960px;padding:2rem 0.5rem;}</style><div class="css-1j83eb7 e1g54axm1"><header class="articleHeader"><style data-emotion-css="1cpgv1a">.css-1cpgv1a{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;}.css-1cpgv1a > .mars{margin-inline-start:0.2em;-webkit-transition:-webkit-transform 0.3s;-webkit-transition:transform 0.3s;transition:transform 0.3s;}.css-1cpgv1a:hover > .mars{-webkit-transform:scale(1.2) rotate(-10deg);-ms-transform:scale(1.2) rotate(-10deg);transform:scale(1.2) rotate(-10deg);}</style><a title="Back to the home page" class="css-1cpgv1a e1gx71j0" href="/"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="chevron-left" class="svg-inline--fa fa-chevron-left fa-w-10 " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentColor" d="M34.52 239.03L228.87 44.69c9.37-9.37 24.57-9.37 33.94 0l22.67 22.67c9.36 9.36 9.37 24.52.04 33.9L131.49 256l154.02 154.75c9.34 9.38 9.32 24.54-.04 33.9l-22.67 22.67c-9.37 9.37-24.57 9.37-33.94 0L34.52 272.97c-9.37-9.37-9.37-24.57 0-33.94z"></path></svg><div class="mars gatsby-image-wrapper" style="position:relative;overflow:hidden;display:inline-block;width:1em;height:1em"><img aria-hidden="true" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAACXBIWXMAAC4jAAAuIwF4pT92AAAFW0lEQVQ4y1WUe0zTVxTHf79fWxBKoU8pLS0U6IM+LM+W8ipUBiIywedEdBFkFpWNIWpwqIg6xRgSfOEjczpUVByKyswgZjr/UIgTcAPdFt18Ydyy/bclFHa/+7Wgcyc5uTf33vPJ99x7z6FGtxVQo+sdlNeebs6jQU0aAPr+xtm2ax9kuzuWZza0lWQ0nCtxuvsqcm3ePe+ZYNYvFGT65p3ZcdTZAidFjdam+ADPNrmYKRb1aL2rrL/CMdhbkoDuRfH4sjge5wpn4HSeGR05JtyYHTf4dVF62evz7TNtvtjPUkyTCy/qs30Lr/aXSp/Vpvb+uTYOA0vZwAVG8u18w3h/UYzndkGU50q2ZrwtJYJcS1HjmwwdLrvierqXL5D5oFkJk4Jebn3HJ/lVS4n0RY19+K9qC16uNnh++zB2YnSNiTypMJOfSo1koFhPvsrRkjMODTkap5poNYV5TsVH4myqaeTsonelXsbJrCSa2jsl+3mto/fvGgteVZvGft9oJaMbEvG42kYeVyWTnyuS8P1iC+4W6Ml3MyPIDbsCHRY5OaibPnbMpMTBRF0P9bY92pC14o8qK55XGjxP11jIi9okMrA6nYyszcCvVWl4sNKB/sXJGCk24mG+BgOZ4eRmfCjpMkjJCY3I06qTo96oKXvzmn3lyYN3FhrwQ6lh4seVVtwuSyXDa9LxpDoTD92Z6F+WgTsLbBiYa8ZIgRaDThVuJrLAWAnORIsm9qmEWKWQDFHmQpr6pS7Hdn6eFadydewDGMntpQmkc4kTw+509JW70F2Si6vznLhTlIShOSbcz4tBX4YKNxJDcSVWQtqjhKRJEYz3xAISyw9Ioe6VO9zHZ5nRlRM1fq9YR3rnxZOL89Nwa0k6js7NRfOcWWjOzcK53GQM52tx16nGzeQw9FpluGQQk7bIENIkF4wXBgdCyuO6qYsL7Q0HXCbstkV7umZqSe9sM9rzk3F5jg2fz0rDpy4nqlIzcTjdir6sSNxyhON6khxdJilORwlxRBWMeinfYwnwA0XTjVT7XHtDU5oR62ZEeQ7bY8hFpx4HMqw47oxDh8uKzmwLDjjMqLPGYqtZgxajAlfNMpzXi3FEHYwmeRBWCAM8IVwuW2T0dqq7KMXdbNdjrV49vsmkIc3x0eRQkhbHbDq02bU4b4vCpWQ1OuKUaNAqUBIux44IMU5GhqAxNIh8LA4giQF+4zTNgKHoSur6omzbQTa4JkZJPtKqyHp9BNkaq8YukxqtlnAcNSvREqvAFwYZzujE2KYSYel0IepCBfDCHIH+ZBqH41VHeAyTQv3DfpvODOPgCaMcO6NDJzbHhOGTKDmpjw5FQ/R0bNHI0BApxR61GHvZ79EgF2ClJAiLQwJJPt8fEh53goWx3YIeculCfVVHXXHFl11gU9uvlXkOR0lIK+stGgnZyaZWpxRhlVyIZbIQlEoEWCjkI08wqUzB4xI2TY8XyNB0+f+qZV+qoWePPgy7lMKxPREisj1ciM1KIdkkDyY1MgFKREGwC/hEHxhAwv39IOVyCIemx3zqaPq/0jvkMPpknnTFybZow0bqVWLUKISeytCQiVUyAamSBpFKUSBKgqchh+9PTP48IuAwbJqUZwr2gMPh+DoOw5oPusOs9k1q1RJpqUzQs0TER3FIIApZLwoOGM/h+3kS/HkeFZczLqBp8NiK5bBO01QPh8v4YCx4ErYvKcY3rtOGvWmw7N2Uh3E5QzIOAzHD4DWE8YJoivBoeoj31p0xUzAWSlG7LWqqRinybbyvENFvXat37m3nbtYbGZpq9KNp9zQOnWKIlrw5xzCvYRTFpk79C0CdTqEKIXTrAAAAAElFTkSuQmCC" alt="" style="position:absolute;top:0;left:0;width:100%;height:100%;object-fit:cover;object-position:center;opacity:1;transition-delay:500ms"/><noscript><picture><source srcset="/static/9381c89176963851c7ecba1737a9acb8/65e33/mars.png 1x,
/static/9381c89176963851c7ecba1737a9acb8/6d161/mars.png 1.5x,
/static/9381c89176963851c7ecba1737a9acb8/69585/mars.png 2x" /><img loading="lazy" width="1em" height="1em" srcset="/static/9381c89176963851c7ecba1737a9acb8/65e33/mars.png 1x,
/static/9381c89176963851c7ecba1737a9acb8/6d161/mars.png 1.5x,
/static/9381c89176963851c7ecba1737a9acb8/69585/mars.png 2x" src="/static/9381c89176963851c7ecba1737a9acb8/65e33/mars.png" alt="" style="position:absolute;top:0;left:0;opacity:1;width:100%;height:100%;object-fit:cover;object-position:center"/></picture></noscript></div></a><h1>The three stages of Explainable AI: How explainability facilitates real world deployment of AI</h1><p><small>Published on<!-- --> <time dateTime="2020-04-22">2020/04/22</time>.</small></p></header><style data-emotion-css="lmbiaj">.css-lmbiaj{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;justify-items:stretch;}.css-lmbiaj .containee{width:100%;max-width:800px;}.css-lmbiaj .page,.css-lmbiaj .controls{position:relative;z-index:10;box-shadow:0 10px 15px -3px rgba(0,0,0,0.2), 0 4px 6px -2px rgba(0,0,0,0.1);}@media print{.css-lmbiaj .page,.css-lmbiaj .controls{box-shadow:none;}}.css-lmbiaj .page{margin:0.5rem 0;}.css-lmbiaj .page .annotationLayer{display:none;}.css-lmbiaj .controls{background:linear-gradient(175deg,#9dc6b1,#7DB397);display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;padding:0 0.5rem;}.css-lmbiaj .controls .title{-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;text-align:center;max-width:70%;margin:1em 0;}.css-lmbiaj .controls .title header{font-size:1.15em;margin:0;white-space:nowrap;overflow:hidden;text-overflow:ellipsis;}</style><article class="css-lmbiaj e19rfvi30"><div class="containee"><div class="react-pdf__Document document"><div class="react-pdf__message react-pdf__message--loading">Loading PDF…</div></div></div></article><blockquote><p>This work matured throughout 2019 in
<a href="#history">various conferences, round tables and workshops</a>, it was formalized
early 2020 to be
<a href="#paper-published-in-the-proceedings-of-the-humains-et-ia-hia-workshop-of-the-20th-extraction-et-gestion-des-connaissances-egc-conference-in-2020">published in the proceedings of EGC</a>.</p></blockquote><p>The explainability of AI has become a major concern for AI builders and users,
especially in the enterprise world. As AIs have more and more impact on the
daily operations of businesses, trust, acceptance, accountability and
certifiability become requirements for any deployment at a large scale.</p><h3 id="explainability-a-key-part-of-the-ai-industry-strategy" style="position:relative"><a aria-label="explainability a key part of the ai industry strategy permalink" class="anchor before" href="#explainability-a-key-part-of-the-ai-industry-strategy"><svg aria-hidden="true" height="20" version="1.1" viewBox="0 0 16 16" width="20"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Explainability, a key part of the AI industry strategy</h3><p>Explainable AI (XAI), as a field, was popularized by the
<a target="_blank" rel="noreferrer noopener" href="https://www.darpa.mil/attachments/XAIProgramUpdate.pdf">eponymous DARPA program</a>
launched in 2017, with the goal of creating a suite of machine learning
techniques that produce more <em>&quot;explainable&quot;</em> models while maintaining a high
level of learning performance, thus enabling human users to understand, trust
and effectively manage the emerging generation of AIs.</p><style data-emotion-css="kvood7">.css-kvood7{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;justify-items:stretch;}.css-kvood7 .containee{width:100%;max-width:800px;}.css-kvood7 .page,.css-kvood7 .controls{position:relative;z-index:10;box-shadow:0 10px 15px -3px rgba(0,0,0,0.2), 0 4px 6px -2px rgba(0,0,0,0.1);}@media print{.css-kvood7 .page,.css-kvood7 .controls{box-shadow:none;}}.css-kvood7 .page{margin:0.5rem 0;}.css-kvood7 .page .annotationLayer{display:none;}.css-kvood7 .controls{background:linear-gradient(175deg,#9dc6b1,#7DB397);display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;padding:0 0.5rem;}.css-kvood7 .controls .title{-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;text-align:center;max-width:70%;margin:1em 0;}.css-kvood7 .controls .title header{font-size:1.15em;margin:0;white-space:nowrap;overflow:hidden;text-overflow:ellipsis;}</style><article class="css-kvood7 e19rfvi30"><div class="containee"><div class="react-pdf__Document document"><div class="react-pdf__message react-pdf__message--loading">Loading PDF…</div></div></div></article><p>Over the past 5 years, explainability has become an important part of the AI
discussions and strategies. Everyone talks about it:</p><ul><li>Research institutions, such as DARPA, in the US, or
<a target="_blank" rel="noreferrer noopener" href="https://www.inria.fr/actualite/actualites-inria/intelligence-artificielle-les-defis-actuels-et-l-action-d-inria">INRIA</a>,
in France, and even governments from the
<a target="_blank" rel="noreferrer noopener" href="https://obamawhitehouse.archives.gov/sites/default/files/whitehouse_files/microsites/ostp/NSTC/preparing_for_the_future_of_ai.pdf">Obama administration</a>
to Macron&#x27;s government through the
<a target="_blank" rel="noreferrer noopener" href="https://www.aiforhumanity.fr/pdfs/9782111457089_Rapport_Villani_accessible.pdf">Villani report</a>
makes it an objective ;</li><li>Big tech corporations, publish open source tools libraries such as
<a target="_blank" rel="noreferrer noopener" href="https://github.com/interpretml/interpret">Microsoft Interpret</a> or
<a target="_blank" rel="noreferrer noopener" href="https://github.com/IBM/AIX360">IBM AI Explainability 360</a> but also release
dedicated services such as
<a target="_blank" rel="noreferrer noopener" href="https://cloud.google.com/explainable-ai">Google Explainable AI</a></li><li>Major data science players, software vendors and services providers alike,
makes it a key part of their offering such as
<a target="_blank" rel="noreferrer noopener" href="https://blog.dataiku.com/announcing-dataiku-7-now-with-deeper-collaboration-and-more-granular-explainability">Dataiku</a>,
<a target="_blank" rel="noreferrer noopener" href="https://www.accenture.com/us-en/insights/technology/explainable-ai-human-machine">Accenture</a>
or <a target="_blank" rel="noreferrer noopener" href="https://www.quantmetry.com/les-livres-blancs/">Quantmetry</a>,</li><li>Startups are launched with explainability as a core benefit such as
<a target="_blank" rel="noreferrer noopener" href="https://www.xplik.ai">xplik</a>, <a target="_blank" rel="noreferrer noopener" href="https://www.dreamquark.com">DreamQuark</a> and of
course, the company I co-created <a target="_blank" rel="noreferrer noopener" href="https://www.craft.ai">craft ai</a>!</li></ul><p>That&#x27;s not all, XAI is gets discussed by AI <em>influencers</em>, MOOCs and conferences
are created to discuss about it!</p><p>Explainability is important, everyone seems to agree!</p><p>This article aims at exploring <strong>why</strong> it is important, what impact
explainability has when deploying AIs in the <em>real</em> world.</p><h3 id="what-is-an-explanation" style="position:relative"><a aria-label="what is an explanation permalink" class="anchor before" href="#what-is-an-explanation"><svg aria-hidden="true" height="20" version="1.1" viewBox="0 0 16 16" width="20"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>What is an explanation?</h3><p>Before talking about Explainable AI, let&#x27;s talk about explanations! XAI is about
providing explanations regarding an AI to its stakeholders, it is therefore
interesting to look at how people explain their decisions to each others. To
help design and understand XAI, we can benefit from the learnings of social
sciences on explanations. That&#x27;s exactly why,
<a target="_blank" rel="noreferrer noopener" href="https://arxiv.org/abs/1706.07269">Tim Miller</a> studied works from various
branches of social sciences from philosophy to cognitive sciences and
psychology.</p><article class="css-lmbiaj e19rfvi30"><div class="containee"><div class="react-pdf__Document document"><div class="react-pdf__message react-pdf__message--loading">Loading PDF…</div></div></div></article><p>The first learning extracted from the surveyed body of work is that people seek
to build a mental model of how stuffs make decisions, how they react to a change
in context, in order to anticipate them and reason about them. Explanations are
a way to build such <em>models</em> much quicker than through observation only.</p><p>Because mental models are inherently subjective, good explanations are biased
towards the explainee to match their perspective and their preexisting
knowledge. In the real world examples we describe in the later sections, we
found that the work of understanding the point of view of the explainee is a
major part of the design of XAIs.</p><p>Another major finding is that good explanations are contrastive. It is not about
answering <em>&quot;why has event E occurred?&quot;</em> but rather <em>&quot;why has event E occurred
instead of event C?&quot;</em>. We found out that the capability to generate such
<em>constrative</em> or <em>counterfactual</em> explanations is quite important in the
deployed systems we describe <a href="#stage-2-explainable-decisions">later</a>.</p><p>Miller argues that Explainable AI as a field should be considered at the
crossroad of Social Science, Human-Computer Interaction and Artificial
Intelligence. Taking a more practical approach, in this article we will take the
point of view of the people and systems interacting with AIs, and study how
explainability impacts these interactions in terms of features, acceptance and
capacity to be deployed.</p><h3 id="non-explainable-ai" style="position:relative"><a aria-label="non explainable ai permalink" class="anchor before" href="#non-explainable-ai"><svg aria-hidden="true" height="20" version="1.1" viewBox="0 0 16 16" width="20"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Non-explainable AI</h3><article class="css-lmbiaj e19rfvi30"><div class="containee"><div class="react-pdf__Document document"><div class="react-pdf__message react-pdf__message--loading">Loading PDF…</div></div></div></article><p>Let&#x27;s do a last detour before actually talking about XAIs, let&#x27;s ponder what
makes an AI non-explainable and also let&#x27;s talk about the difference between
explainability and determinism.</p><p>Neural networks (NN) are often considered to be the epitome of black boxes, of
non explainable AIs. To understand why you need basic understanding of
<a target="_blank" rel="noreferrer noopener" href="https://www.superdatascience.com/blogs/artificial-neural-networks-how-do-neural-networks-work">how NNs work</a>:
each neuron in the network computes a linear combination of its input variables
using learned coefficient and then applies an activation function to the result,
each layer of neurons works on the previous layer output, up until the actual
input variables. The result of the NN from a given input is therefore basically
a linear combination of linear combinations, of ... of the input variables.</p><p>This basically means two things:</p><ol><li>The NN computation can be written down as a (large) equation and computed by
hand, there is no magic, just maths, from a given input the output will
always be the same, a NN is completely deterministic ;</li><li>Unless the NN is really small, it is really difficult to understand how a
variation in the input will impact the output: weights interactions are
difficult to grasp and dimensional analysis is not possible, as the
computation just manipulate numbers with no regards to their quantity.</li></ol><p>In short, basic maths makes it easy to apply a NN to an input, the nature of the
computation makes the output very difficult to predict its behavior. That what
makes NN non-explainable, even the best specialists have a hard time creating
mental model for a trained neural network.</p><article class="css-lmbiaj e19rfvi30"><div class="containee"><div class="react-pdf__Document document"><div class="react-pdf__message react-pdf__message--loading">Loading PDF…</div></div></div></article><p>The asymmetry between easy to compute but difficult to understand makes it a
good target for <em>adversarial attacks</em>. An automated process analyze the NN
equation to find <em>weak spots</em> that couldn&#x27;t be spotted by the designers of the
NN. Those weak spots enables the attacking process to create subtle change to
the input that are invisible to human experts but confuses the NN. For instance,
one can make a NN powered vision AI mistake a washer for a loudspeaker by
changing only a few pixels. In this case the lack of explainability makes it
impossible for a human too predict defects in the deployed AI.</p><h2 id="how-xai-makes-a-difference" style="position:relative"><a aria-label="how xai makes a difference permalink" class="anchor before" href="#how-xai-makes-a-difference"><svg aria-hidden="true" height="20" version="1.1" viewBox="0 0 16 16" width="20"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>How XAI makes a difference</h2><p>In order to study the impact explainability makes on AI projects we are
categorizing effects in three stages. Higher stages require higher levels of
explainability and have more impact on the resulting AIs. We take the point of
view of the industrial world, and look at how explainability can make a
difference in the deployment and application of AI.</p><p>This work is based on the experience we gathered working and discussing with our
customers, partners and community, as a provider of machine learning solutions.
Examples are focused on systems based on Machine Learning but the proposed three
stages are relevant to any kind of AI.</p><h3 id="stage-1-explainable-building-process" style="position:relative"><a aria-label="stage 1 explainable building process permalink" class="anchor before" href="#stage-1-explainable-building-process"><svg aria-hidden="true" height="20" version="1.1" viewBox="0 0 16 16" width="20"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Stage 1: Explainable building process</h3><article class="css-lmbiaj e19rfvi30"><div class="containee"><div class="react-pdf__Document document"><div class="react-pdf__message react-pdf__message--loading">Loading PDF…</div></div></div></article><p>In any organisation, just like any IT project, a project leveraging AI aims to
have an impact on the daily job of some people. Its goal might even directly be
to automate part of worker&#x27;s job or to help them deliver value they could not
before. Especially when AI is involved, affected users can be wary of the new
system. In particular they may feel threatened by the automation of some of
their tasks, or may not believe that a simple <em>computer program</em> can execute
complex tasks correctly. A recommandable method to address those concerns is to
involve them in the building of the AI. This is where explainability plays a big
role.</p><article class="css-lmbiaj e19rfvi30"><div class="containee"><div class="react-pdf__Document document"><div class="react-pdf__message react-pdf__message--loading">Loading PDF…</div></div></div></article><p>In this context, traditional quality metrics such as confusion matrices, r2,
RMSE, MAE, etc. are not sufficient to get the future AI user&#x27;s trust, since they
want to know more about the <em>why</em> than about the raw results. Visualization is
the first go-to technique. Simply plotting the output against context variables
is a good way to get a <em>feel</em> for how an AI performs over the target domain when
dimensionality is low. Interactive simulations can help explore the domain to
experience how the AI will react. Beyond these techniques which are applicable
to any <em>black box</em> computations, more advanced techniques open the hood and make
the structure of the AI itself inspectable.</p><p>In the following sections describing the subsequent stages, we will talk about
techniques able to work while the AIs are <em>live</em>, processing production data, at
production speed. These techniques are also well suited for stage 1, where the
inspection is offline, with less data and runtime constraints.</p><article class="css-lmbiaj e19rfvi30"><div class="containee"><div class="react-pdf__Document document"><div class="react-pdf__message react-pdf__message--loading">Loading PDF…</div></div></div></article><p>Debugging tools that were initially designed for data scientists can also be
leveraged for other stakeholders. AIs powered by neural networks can be
inspected by visualizing how intermediate layers <em>react</em> to different input,
<a target="_blank" rel="noreferrer noopener" href="https://playground.tensorflow.org">Tensorflow Playground</a> or
<a target="_blank" rel="noreferrer noopener" href="https://cs.stanford.edu/people/karpathy/convnetjs/docs.html">ConvnetJS</a> are
good examples of this approach. On images, the computation of
<a target="_blank" rel="noreferrer noopener" href="https://arxiv.org/abs/1312.6034"><em>saliency maps</em></a> can also help to convey which
parts of the image are considered by the network to make its prediction. This
technique led to the identification of the infamous
<a target="_blank" rel="noreferrer noopener" href="https://arxiv.org/abs/1602.04938"><em>husky vs wolf</em></a> issue in which a wolf is
primarily identified by the presence of snow in the picture. Tools like
<a target="_blank" rel="noreferrer noopener" href="https://seq2seq-vis.io">Seq2Seq Vis</a> bring the same kind of debugging
capabilities to natural language focused neural networks. This shows that even
neural networks, which are considered black boxes, can be at least partly
explained offline to the non-technical AI project stakeholder by using the right
tools.</p><p>While the initial goal of explaining why the AI works the way it does is to ease
its adoption, explainability also increases the involvement of potential users
by letting them achieve a deeper understanding. As a result they can assist in
its development, ensuring that the AI solves an actual problem, and provide
valuable feedback on specific behaviors of the AI: instead of providing
knowledge upfront, it is always easier to <em>react</em> to what you see the AI doing
and why it does it. In many cases, domain experts can easily help if they have
an understanding of why the AI makes decisions: sensors having an undocumented
validity domain, well-known contexts leading to corrupted data, spurious
correlations because of a missing data sources, etc.</p><p>The first stage of explainability is about helping create a multi disciplinary
team of experts in their respective fields who understand the AI they are
building. Offline explainability techniques are key to the acceptance of the
future AI and create opportunities to build a better system.</p><h3 id="stage-2-explainable-decisions" style="position:relative"><a aria-label="stage 2 explainable decisions permalink" class="anchor before" href="#stage-2-explainable-decisions"><svg aria-hidden="true" height="20" version="1.1" viewBox="0 0 16 16" width="20"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Stage 2: Explainable decisions</h3><article class="css-lmbiaj e19rfvi30"><div class="containee"><div class="react-pdf__Document document"><div class="react-pdf__message react-pdf__message--loading">Loading PDF…</div></div></div></article><p>Trust in a system is key, especially in an enterprise tool that has an impact on
day to day business. Trust makes the difference between a system that is <em>&quot;micro
managed&quot;</em> by its users or supervisors, and a system that can enjoy a larger
autonomy. The more management a system needs, the more manpower it requires and
therefore the less value it has.</p><p>Trust is built when a system is not surprising, when it behaves according to our
mental model. A system whose limits are understood by its users is arguably more
valuable than a more accurate system whose results are considered unreliable. As
discussed <a href="#what-is-an-explanation">above</a>, explanations are a good way to
accelerate the construction of this mental model. That is where the capacity to
explain the AIs&#x27; decisions has an impact. That is the second stage of
explainable AI.</p><p>Stage 1 explainability does not have the same impact: most users or supervisors
of AIs did not have the chance to participate in their inception, and in more
and more cases, AIs can evolve over time. Furthermore, the ability to access
explanations of past AI decisions can help pinpoint root causes and generally
provide traceability.</p><article class="css-lmbiaj e19rfvi30"><div class="containee"><div class="react-pdf__Document document"><div class="react-pdf__message react-pdf__message--loading">Loading PDF…</div></div></div></article><p>The ability to provide explanations to any AI decision is an active field of
innovation with methods such as
<a target="_blank" rel="noreferrer noopener" href="http://blog.datadive.net/random-forest-interpretation-with-scikit-learn">TreeInterpreter</a>
<a target="_blank" rel="noreferrer noopener" href="https://arxiv.org/abs/1606.05386">LIME</a> or
<a target="_blank" rel="noreferrer noopener" href="https://arxiv.org/abs/1705.07874">SHAP</a>. Given a predictive model and a
prediction, these methods aim at providing a local explanation for the
prediction. This explanation takes the form of linear factors that can be
applied to input features to reach the predicted results, thus giving an idea of
the local feature importance and behavior of the model. The computed feature
factors can also be used to generated counterfactual examples and give an idea
of the trend of the predicted value given changes in the input features.</p><p>An interesting property of this class of algorithm is that they can work using a
feature set that is different from the actual feature set used by the model. It
is therefore possible to adapt the explanation by making it more comprehensible
to the explainee, independently from the features that yield the best
predictions. This additional feature engineering step is not without risk,
<a target="_blank" rel="noreferrer noopener" href="https://hal.sorbonne-universite.fr/hal-02184519">Christophe Denis and Frank Varenne</a>
argue, it can be used to convince explainees to blindly trust said AI, by
presenting a deceptive approximation instead of bringing more transparency.</p><article class="css-lmbiaj e19rfvi30"><div class="containee"><div class="react-pdf__Document document"><div class="react-pdf__message react-pdf__message--loading">Loading PDF…</div></div></div></article><p>A good example of SHAP usage can be found in the banking fraud detection
solution provided by
<a target="_blank" rel="noreferrer noopener" href="https://www.craft.ai/blog/ai-night-2019-explainable-ai-workshop">Bleckwen</a>
company. One key part of the solution is a predictive model, trained on labelled
datasets containing fraudulent and non-fraudulent transactions. This model
computes a score for each transactions. Transactions having a score above a
certain threshold are reviewed by a human expert to confirm their fraudulent
nature. One of their customers&#x27; requirements is to get explanations for every
score. They chose to use non-explainable gradient boosting techniques for the
model on a range of complex features. The local explanation is computed by SHAP
on a range of features they designed with their end users to make them
completely understandable to them.</p><article class="css-lmbiaj e19rfvi30"><div class="containee"><div class="react-pdf__Document document"><div class="react-pdf__message react-pdf__message--loading">Loading PDF…</div></div></div></article><p>Another example of stage 2 explainable AI is how
<a target="_blank" rel="noreferrer noopener" href="https://www.craft.ai/blog/client-ia-dalkia">Dalkia uses machine learning as a part of their energy management dashboard</a>.
Here, decision trees are used to predict an energy diagnosis based on labelled
data streams. Predictions are used as diagnosis recommendations in the energy
managers&#x27; dashboard, and explanations are extracted from the decision tree as a
set of rules that were applied. What&#x27;s really interesting in this example is
that without explanations alongside the recommendations this AI would not have
any value. At its core, the goal of the system is to help energy managers handle
more data points. Without an explanation, when provided with a prediction,
energy managers would need to investigate the raw data in order to confirm or
contradict it. They would end up doing the same amount of work as without
explanations. When an explanation is provided, this counter investigation is
only needed when the energy manager disagrees with it. Here, explanations are
needed for the business value of the AI.</p><h3 id="stage-3-explainable-decision-process" style="position:relative"><a aria-label="stage 3 explainable decision process permalink" class="anchor before" href="#stage-3-explainable-decision-process"><svg aria-hidden="true" height="20" version="1.1" viewBox="0 0 16 16" width="20"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Stage 3: Explainable decision process</h3><article class="css-lmbiaj e19rfvi30"><div class="containee"><div class="react-pdf__Document document"><div class="react-pdf__message react-pdf__message--loading">Loading PDF…</div></div></div></article><p>Stages 1 and 2 are about helping humans create a mental model of how AIs
operate. This enables humans to <em>&quot;reason&quot;</em> about the way AIs work critically,
and decide when to trust them and accept their outputs, predictions or
recommendations. To scale this up to many AIs and over time, you need to define
business logic that will apply the same <em>&quot;reasoning&quot;</em> automatically. Stage 3 is
about enabling interoperability between AIs and other pieces of software,
especially software that uses business logic.</p><p>When discussing AI, and especially models generated through machine learning, we
often talk about the underlying concepts they capture, for example convolutional
neural networks are able to recognize visual patterns and build upon these lower
level <em>&quot;concepts&quot;</em> in their predictions. AIs that can explain those lower level
building blocks, make them inspectable to business logic, reach stage 3. Such
AIs ultimately act as a knowledge base of the behavior they model.</p><p>Stage 3 explainability makes a difference especially when a lot of instances of
evolving AIs need to be supervised by business logic, for example in a context
of continuous certifiability or collaborative automation between machine
learning based AIs and business rules.</p><article class="css-lmbiaj e19rfvi30"><div class="containee"><div class="react-pdf__Document document"><div class="react-pdf__message react-pdf__message--loading">Loading PDF…</div></div></div></article><p>This level of explainability requires fully explainable AI. Machine learning
techniques such as linear regressions or decision tree learning can reach such
levels. Another approach is to approximate a more <em>&quot;black box&quot;</em> model with a
more explainable model, for example <a target="_blank" rel="noreferrer noopener" href="https://arxiv.org/abs/0811.1679">RuleFit</a>
is able to learn a minimal ensemble of rules from a tree ensemble method such as
Random Forest.</p><article class="css-lmbiaj e19rfvi30"><div class="containee"><div class="react-pdf__Document document"><div class="react-pdf__message react-pdf__message--loading">Loading PDF…</div></div></div></article><p>An interesting example of level 3 explainability is
<a target="_blank" rel="noreferrer noopener" href="https://www.craft.ai/blog/how-total-direct-energie-applies-explainable-ai-to-its-virtual-assistant">Total Direct Energie&#x27;s energy coaching feature</a>
that is part of their customer-facing mobile application. It generates
personalized messages for each customer. At its core, the system is made of a
machine learning-based energy consumption predictive model, and a business
expertise-based message generation and selection module. The predictive model is
made of individual regression trees, each updated continuously from the data of
a single household. The message generation module is generic for all users, and
uses the model&#x27;s explanations and predictions as input data to select and
personalize each message. So the predictive models provide an understanding of
the household&#x27;s energy consumption behavior, which is automatically processed to
generate personalized messages.</p><p>When presented with a visual explanation of a decision process, people tend to
navigate through its structure to understand the process. Stage 3 is about
letting software programs, other AIs, do the same thing, thus unlocking a wealth
of additional use cases.</p><h2 id="challenges" style="position:relative"><a aria-label="challenges permalink" class="anchor before" href="#challenges"><svg aria-hidden="true" height="20" version="1.1" viewBox="0 0 16 16" width="20"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Challenges</h2><p>While there are already deployed AIs covering these three stages, there are
still challenges ahead before explainable AI can be generalized.</p><h3 id="evaluating-explanations" style="position:relative"><a aria-label="evaluating explanations permalink" class="anchor before" href="#evaluating-explanations"><svg aria-hidden="true" height="20" version="1.1" viewBox="0 0 16 16" width="20"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Evaluating explanations</h3><article class="css-lmbiaj e19rfvi30"><div class="containee"><div class="react-pdf__Document document"><div class="react-pdf__message react-pdf__message--loading">Loading PDF…</div></div></div></article><p>In the previous sections we discussed how certain techniques bring more or less
explainability, however we did not discuss how we can make such an assessment.</p><p>Ad-hoc experiments or KPI can be used. For example the D-Edge company, which
provides pricing recommendations to hotel managers among other services,
measures whether explained recommendations are accepted. Every recommendation is
accompanied by a natural language explanation. Managers can accept and apply the
recommendation to their pricing or discard it. As presented during a
<a target="_blank" rel="noreferrer noopener" href="https://www.craft.ai/blog/ai-night-2019-explainable-ai-workshop">round table focused on XAI</a>,
they consider the proportion of accepted recommendations as a proxy measure for
the quality of their explanation. We believe that this makes sense, as hotel
managers need to be convinced to make such an impactful change to their
business.</p><p>In the general case, other proxy measures can be used, such as the number of
rules, nodes or input variables considered in an explanation or explainable
model. However these lack generality: how can the explainability of a linear
regression and of a regression tree be compared? They also lack an experimental,
measurable ground truth: for example we do not know if humans find that the
explainability provided by LIME grows exponentially or linearly with the number
of features involved. Furthermore, as discussed
<a href="#what-is-an-explanation">above</a>, what constitutes a good or a bad explanation
depends on the recipient of the explanation and their own cognitive biases. This
poses an additional challenge to this evaluation. There is a lack of a systemic
framework or objective criteria to evaluate the explanations provided by AIs. A
challenge <a target="_blank" rel="noreferrer noopener" href="https://arxiv.org/abs/1708.01870">identified by Adrian Weller</a>.</p><h3 id="improving-the-performances-of-xai" style="position:relative"><a aria-label="improving the performances of xai permalink" class="anchor before" href="#improving-the-performances-of-xai"><svg aria-hidden="true" height="20" version="1.1" viewBox="0 0 16 16" width="20"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Improving the performances of XAI</h3><article class="css-lmbiaj e19rfvi30"><div class="containee"><div class="react-pdf__Document document"><div class="react-pdf__message react-pdf__message--loading">Loading PDF…</div></div></div></article><p>The AI community generally considers that the more explainability you gain, the
less predictive performance you can achieve, especially in Machine Learning.
Overcoming this is a primary goal of the XAI field, and in particular it is the
main goal of the DARPA XAI program.</p><p>Several opportunities have been identified to achieve this objective, the most
promising ones being to create <strong>hybrid AIs</strong> combining different approaches.</p><p>One idea is to <em>push</em> high-performance but unexplainable algorithms to the
edges, around an explainable core. For example a deep neural networks would
identify low level details like whiskers and pointy ears, while decision trees
or bayesian models would associate the presence of both whiskers and pointy ears
to a cat in an explainable fashion.</p><p>Another idea is to adapt Machine Learning algorithms to work from existing
expert-built symbolic representations of physical models to leverage existing
knowledge, instead of having to relearn and embed it. This field is relatively
new, and comes as a stark departure from the deep learning trend of the past few
years. This is at the heart of
<a target="_blank" rel="noreferrer noopener" href="https://www.irt-systemx.fr/systemx-lance-le-programme-de-recherche-intelligence-artificielle-et-ingenierie-augmentee-ia2/">IRT SystemX IA2 program</a>.</p><h2 id="conclusion" style="position:relative"><a aria-label="conclusion permalink" class="anchor before" href="#conclusion"><svg aria-hidden="true" height="20" version="1.1" viewBox="0 0 16 16" width="20"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Conclusion</h2><article class="css-lmbiaj e19rfvi30"><div class="containee"><div class="react-pdf__Document document"><div class="react-pdf__message react-pdf__message--loading">Loading PDF…</div></div></div></article><p>We structured in three stages the impact that explainability has on AIs deployed
in the <em>&quot;real world&quot;</em>. Those 3 stages provide a simple framework to quickly
identify the need for explainability in a AI powered project.</p><p><strong>Stage 1</strong> is about leveraging explainability to improve the adoption and
performance of AIs.</p><p><strong>Stage 2</strong> is about explaining every AI decisions to build trust with their
users and supervisors.</p><p><strong>Stage 3</strong> is about enabling the interoperability of AIs with each other and
other software, thus unlocking new and richer use cases.</p><article class="css-lmbiaj e19rfvi30"><div class="containee"><div class="react-pdf__Document document"><div class="react-pdf__message react-pdf__message--loading">Loading PDF…</div></div></div></article><p>Because we focused on what explainability enables in AI, we did not discuss
regulation. However it is important to note that initiatives such as the
European GDPR pave the way for a
<a target="_blank" rel="noreferrer noopener" href="https://iapp.org/news/a/is-there-a-right-to-explanation-for-machine-learning-in-the-gdpr"><em>&quot;right to explanation&quot;</em></a>
which will require, at least in some cases, a stage 2 requirement. We strongly
believe that stage 2 explainability is a key to actually operationalize
enterprise AI because it not only offers stronger guarantees in terms of data
governance, but also facilitates involvement and support from users and domain
experts impacted by such AI.</p><p><strong>Far from being just a constraint on AI design, explainability is actually an
opportunity to develop AIs that actually deliver value to Humans.</strong></p><article class="css-lmbiaj e19rfvi30"><div class="containee"><div class="react-pdf__Document document"><div class="react-pdf__message react-pdf__message--loading">Loading PDF…</div></div></div></article><hr/><blockquote><p>Paper, as published in the proceedings of the &quot;Humains et IA (HIA)&quot; workshop
of the
<a target="_blank" rel="noreferrer noopener" href="https://egc2020.sciencesconf.org">20th Extraction et Gestion des Connaissances (EGC) conference in 2020</a>.</p></blockquote><article class="css-kvood7 e19rfvi30"><div class="containee"><div class="react-pdf__Document document"><div class="react-pdf__message react-pdf__message--loading">Loading PDF…</div></div></div></article><h2 id="history" style="position:relative"><a aria-label="history permalink" class="anchor before" href="#history"><svg aria-hidden="true" height="20" version="1.1" viewBox="0 0 16 16" width="20"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>History</h2><ul><li><strong>2019/03/13</strong> -
<a target="_blank" rel="noreferrer noopener" href="https://nuit-blanche.blogspot.com/2019/03/ce-soir-paris-machine-learning-5-season.html">Paris Machine Learning Meetup #5, season 6</a>.</li><li><strong>2019/04/18</strong> -
<a target="_blank" rel="noreferrer noopener" href="https://www.craft.ai/blog/ai-night-2019-explainable-ai-workshop">AI Night <em>explainability</em> roundtable</a>.</li><li><strong>2019/05/24</strong> &amp; <strong>2019/07/18</strong> -
<a target="_blank" rel="noreferrer noopener" href="https://sc21.fr/total-retour-sur-la-session-aiforleaders-du-24-mai/">#AIforLeaders workshops at Total</a>.</li><li><strong>2019/10/07</strong> -
<a target="_blank" rel="noreferrer noopener" href="https://ax.polytechnique.org/newsletter/preview/index/id/503/noLayout/1">X-IA #6 <em>Intelligibilité des modèles</em> Meetup</a></li><li><strong>2019/10/17</strong> -
<a target="_blank" rel="noreferrer noopener" href="https://www.association-aristote.fr/lia-est-elle-explicable/">Séminaire Aristote, l’IA est-elle explicable ? Un coup d&#x27;oeil furtif dans la boite noire des algorithmes de l&#x27;IA</a>.</li><li><strong>2020/01/28</strong> -
<a target="_blank" rel="noreferrer noopener" href="http://headwork.gforge.inria.fr/HIA2020/index">Humains et IA (HIA) workshop</a>
of the 20th Extraction et Gestion des Connaissances (EGC) conference.</li></ul><footer class="articleFooter"><a title="Back to the home page" class="css-1cpgv1a e1gx71j0" href="/"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="chevron-left" class="svg-inline--fa fa-chevron-left fa-w-10 " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentColor" d="M34.52 239.03L228.87 44.69c9.37-9.37 24.57-9.37 33.94 0l22.67 22.67c9.36 9.36 9.37 24.52.04 33.9L131.49 256l154.02 154.75c9.34 9.38 9.32 24.54-.04 33.9l-22.67 22.67c-9.37 9.37-24.57 9.37-33.94 0L34.52 272.97c-9.37-9.37-9.37-24.57 0-33.94z"></path></svg><div class="mars gatsby-image-wrapper" style="position:relative;overflow:hidden;display:inline-block;width:1em;height:1em"><img aria-hidden="true" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAACXBIWXMAAC4jAAAuIwF4pT92AAAFW0lEQVQ4y1WUe0zTVxTHf79fWxBKoU8pLS0U6IM+LM+W8ipUBiIywedEdBFkFpWNIWpwqIg6xRgSfOEjczpUVByKyswgZjr/UIgTcAPdFt18Ydyy/bclFHa/+7Wgcyc5uTf33vPJ99x7z6FGtxVQo+sdlNeebs6jQU0aAPr+xtm2ax9kuzuWZza0lWQ0nCtxuvsqcm3ePe+ZYNYvFGT65p3ZcdTZAidFjdam+ADPNrmYKRb1aL2rrL/CMdhbkoDuRfH4sjge5wpn4HSeGR05JtyYHTf4dVF62evz7TNtvtjPUkyTCy/qs30Lr/aXSp/Vpvb+uTYOA0vZwAVG8u18w3h/UYzndkGU50q2ZrwtJYJcS1HjmwwdLrvierqXL5D5oFkJk4Jebn3HJ/lVS4n0RY19+K9qC16uNnh++zB2YnSNiTypMJOfSo1koFhPvsrRkjMODTkap5poNYV5TsVH4myqaeTsonelXsbJrCSa2jsl+3mto/fvGgteVZvGft9oJaMbEvG42kYeVyWTnyuS8P1iC+4W6Ml3MyPIDbsCHRY5OaibPnbMpMTBRF0P9bY92pC14o8qK55XGjxP11jIi9okMrA6nYyszcCvVWl4sNKB/sXJGCk24mG+BgOZ4eRmfCjpMkjJCY3I06qTo96oKXvzmn3lyYN3FhrwQ6lh4seVVtwuSyXDa9LxpDoTD92Z6F+WgTsLbBiYa8ZIgRaDThVuJrLAWAnORIsm9qmEWKWQDFHmQpr6pS7Hdn6eFadydewDGMntpQmkc4kTw+509JW70F2Si6vznLhTlIShOSbcz4tBX4YKNxJDcSVWQtqjhKRJEYz3xAISyw9Ioe6VO9zHZ5nRlRM1fq9YR3rnxZOL89Nwa0k6js7NRfOcWWjOzcK53GQM52tx16nGzeQw9FpluGQQk7bIENIkF4wXBgdCyuO6qYsL7Q0HXCbstkV7umZqSe9sM9rzk3F5jg2fz0rDpy4nqlIzcTjdir6sSNxyhON6khxdJilORwlxRBWMeinfYwnwA0XTjVT7XHtDU5oR62ZEeQ7bY8hFpx4HMqw47oxDh8uKzmwLDjjMqLPGYqtZgxajAlfNMpzXi3FEHYwmeRBWCAM8IVwuW2T0dqq7KMXdbNdjrV49vsmkIc3x0eRQkhbHbDq02bU4b4vCpWQ1OuKUaNAqUBIux44IMU5GhqAxNIh8LA4giQF+4zTNgKHoSur6omzbQTa4JkZJPtKqyHp9BNkaq8YukxqtlnAcNSvREqvAFwYZzujE2KYSYel0IepCBfDCHIH+ZBqH41VHeAyTQv3DfpvODOPgCaMcO6NDJzbHhOGTKDmpjw5FQ/R0bNHI0BApxR61GHvZ79EgF2ClJAiLQwJJPt8fEh53goWx3YIeculCfVVHXXHFl11gU9uvlXkOR0lIK+stGgnZyaZWpxRhlVyIZbIQlEoEWCjkI08wqUzB4xI2TY8XyNB0+f+qZV+qoWePPgy7lMKxPREisj1ciM1KIdkkDyY1MgFKREGwC/hEHxhAwv39IOVyCIemx3zqaPq/0jvkMPpknnTFybZow0bqVWLUKISeytCQiVUyAamSBpFKUSBKgqchh+9PTP48IuAwbJqUZwr2gMPh+DoOw5oPusOs9k1q1RJpqUzQs0TER3FIIApZLwoOGM/h+3kS/HkeFZczLqBp8NiK5bBO01QPh8v4YCx4ErYvKcY3rtOGvWmw7N2Uh3E5QzIOAzHD4DWE8YJoivBoeoj31p0xUzAWSlG7LWqqRinybbyvENFvXat37m3nbtYbGZpq9KNp9zQOnWKIlrw5xzCvYRTFpk79C0CdTqEKIXTrAAAAAElFTkSuQmCC" alt="" style="position:absolute;top:0;left:0;width:100%;height:100%;object-fit:cover;object-position:center;opacity:1;transition-delay:500ms"/><noscript><picture><source srcset="/static/9381c89176963851c7ecba1737a9acb8/65e33/mars.png 1x,
/static/9381c89176963851c7ecba1737a9acb8/6d161/mars.png 1.5x,
/static/9381c89176963851c7ecba1737a9acb8/69585/mars.png 2x" /><img loading="lazy" width="1em" height="1em" srcset="/static/9381c89176963851c7ecba1737a9acb8/65e33/mars.png 1x,
/static/9381c89176963851c7ecba1737a9acb8/6d161/mars.png 1.5x,
/static/9381c89176963851c7ecba1737a9acb8/69585/mars.png 2x" src="/static/9381c89176963851c7ecba1737a9acb8/65e33/mars.png" alt="" style="position:absolute;top:0;left:0;opacity:1;width:100%;height:100%;object-fit:cover;object-position:center"/></picture></noscript></div></a></footer></div></article></main><style data-emotion-css="16r7bwy">.css-16r7bwy{margin:0 auto;padding:0 0.5rem;max-width:960px;color:#FFFFFF;padding-top:1rem;padding-bottome:1rem;}.css-16r7bwy nav{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;}.css-16r7bwy nav ul{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;padding:0;}.css-16r7bwy nav ul li{margin-top:0.5em;margin-bottom:0.5em;margin-left:1em;display:block;list-style:none;}.css-16r7bwy nav ul li:after{content:"•";margin-left:1em;}.css-16r7bwy nav ul li:last-child:after{content:none;}.css-16r7bwy .license{font-size:0.7em;}@media print{.css-16r7bwy{display:none;}}</style><footer class="css-16r7bwy e1xeehci0"><nav><ul><li><a href="/">Home</a></li><li><a href="/articles">Articles</a></li><li><a href="/content/resume.en">Resume</a></li></ul></nav><p class="license"><svg aria-hidden="true" focusable="false" data-prefix="fab" data-icon="creative-commons" class="svg-inline--fa fa-creative-commons fa-w-16 " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path fill="currentColor" d="M245.83 214.87l-33.22 17.28c-9.43-19.58-25.24-19.93-27.46-19.93-22.13 0-33.22 14.61-33.22 43.84 0 23.57 9.21 43.84 33.22 43.84 14.47 0 24.65-7.09 30.57-21.26l30.55 15.5c-6.17 11.51-25.69 38.98-65.1 38.98-22.6 0-73.96-10.32-73.96-77.05 0-58.69 43-77.06 72.63-77.06 30.72-.01 52.7 11.95 65.99 35.86zm143.05 0l-32.78 17.28c-9.5-19.77-25.72-19.93-27.9-19.93-22.14 0-33.22 14.61-33.22 43.84 0 23.55 9.23 43.84 33.22 43.84 14.45 0 24.65-7.09 30.54-21.26l31 15.5c-2.1 3.75-21.39 38.98-65.09 38.98-22.69 0-73.96-9.87-73.96-77.05 0-58.67 42.97-77.06 72.63-77.06 30.71-.01 52.58 11.95 65.56 35.86zM247.56 8.05C104.74 8.05 0 123.11 0 256.05c0 138.49 113.6 248 247.56 248 129.93 0 248.44-100.87 248.44-248 0-137.87-106.62-248-248.44-248zm.87 450.81c-112.54 0-203.7-93.04-203.7-202.81 0-105.42 85.43-203.27 203.72-203.27 112.53 0 202.82 89.46 202.82 203.26-.01 121.69-99.68 202.82-202.84 202.82z"></path></svg> <svg aria-hidden="true" focusable="false" data-prefix="fab" data-icon="creative-commons-by" class="svg-inline--fa fa-creative-commons-by fa-w-16 " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path fill="currentColor" d="M314.9 194.4v101.4h-28.3v120.5h-77.1V295.9h-28.3V194.4c0-4.4 1.6-8.2 4.6-11.3 3.1-3.1 6.9-4.7 11.3-4.7H299c4.1 0 7.8 1.6 11.1 4.7 3.1 3.2 4.8 6.9 4.8 11.3zm-101.5-63.7c0-23.3 11.5-35 34.5-35s34.5 11.7 34.5 35c0 23-11.5 34.5-34.5 34.5s-34.5-11.5-34.5-34.5zM247.6 8C389.4 8 496 118.1 496 256c0 147.1-118.5 248-248.4 248C113.6 504 0 394.5 0 256 0 123.1 104.7 8 247.6 8zm.8 44.7C130.2 52.7 44.7 150.6 44.7 256c0 109.8 91.2 202.8 203.7 202.8 103.2 0 202.8-81.1 202.8-202.8.1-113.8-90.2-203.3-202.8-203.3z"></path></svg> The content on this website, of which Clodéric Mars is the author, is licensed under a<!-- --> <a target="_blank" rel="noreferrer noopener" href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International license</a>.</p><p class="license"><svg aria-hidden="true" focusable="false" data-prefix="fab" data-icon="osi" class="svg-inline--fa fa-osi fa-w-16 " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M8 266.44C10.3 130.64 105.4 34 221.8 18.34c138.8-18.6 255.6 75.8 278 201.1 21.3 118.8-44 230-151.6 274-9.3 3.8-14.4 1.7-18-7.7q-26.7-69.45-53.4-139c-3.1-8.1-1-13.2 7-16.8 24.2-11 39.3-29.4 43.3-55.8a71.47 71.47 0 0 0-64.5-82.2c-39-3.4-71.8 23.7-77.5 59.7-5.2 33 11.1 63.7 41.9 77.7 9.6 4.4 11.5 8.6 7.8 18.4q-26.85 69.9-53.7 139.9c-2.6 6.9-8.3 9.3-15.5 6.5-52.6-20.3-101.4-61-130.8-119-24.9-49.2-25.2-87.7-26.8-108.7zm20.9-1.9c.4 6.6.6 14.3 1.3 22.1 6.3 71.9 49.6 143.5 131 183.1 3.2 1.5 4.4.8 5.6-2.3q22.35-58.65 45-117.3c1.3-3.3.6-4.8-2.4-6.7-31.6-19.9-47.3-48.5-45.6-86 1-21.6 9.3-40.5 23.8-56.3 30-32.7 77-39.8 115.5-17.6a91.64 91.64 0 0 1 45.2 90.4c-3.6 30.6-19.3 53.9-45.7 69.8-2.7 1.6-3.5 2.9-2.3 6q22.8 58.8 45.2 117.7c1.2 3.1 2.4 3.8 5.6 2.3 35.5-16.6 65.2-40.3 88.1-72 34.8-48.2 49.1-101.9 42.3-161-13.7-117.5-119.4-214.8-255.5-198-106.1 13-195.3 102.5-197.1 225.8z"></path></svg> The sources of this website, of which Clodéric Mars is the author, are licensed under a<!-- --> <a target="_blank" rel="noreferrer noopener" href="https://choosealicense.com/licenses/mit/">MIT License</a> <!-- -->and are available on<!-- --> <a target="_blank" rel="noreferrer noopener" href="https://github.com/cloderic/www">Github</a>.</p></footer></div><div id="gatsby-announcer" style="position:absolute;top:0;width:1px;height:1px;padding:0;overflow:hidden;clip:rect(0, 0, 0, 0);white-space:nowrap;border:0" aria-live="assertive" aria-atomic="true"></div></div><script>
  
  function gaOptout(){document.cookie=disableStr+'=true; expires=Thu, 31 Dec 2099 23:59:59 UTC;path=/',window[disableStr]=!0}var gaProperty='UA-3838411-6',disableStr='ga-disable-'+gaProperty;document.cookie.indexOf(disableStr+'=true')>-1&&(window[disableStr]=!0);
  if(!(parseInt(navigator.doNotTrack) === 1 || parseInt(window.doNotTrack) === 1 || parseInt(navigator.msDoNotTrack) === 1 || navigator.doNotTrack === "yes")) {
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  }
  if (typeof ga === "function") {
    ga('create', 'UA-3838411-6', 'auto', {});
      ga('set', 'anonymizeIp', true);
      
      
      
      
      }</script><script id="gatsby-script-loader">/*<![CDATA[*/window.pagePath="/articles/2020-04-22-the-three-stages-of-explainable-ai/";/*]]>*/</script><script id="gatsby-chunk-mapping">/*<![CDATA[*/window.___chunkMapping={"app":["/app-ad690ddceb3d8eaa4fb6.js"],"pdfjsWorker":["/pdfjsWorker-54425af40629fc655008.js"],"component---src-pages-404-mdx":["/component---src-pages-404-mdx-6afbd8aeba80f8b4582d.js"],"component---src-pages-articles-2009-07-29-first-post-manifesto-mdx":["/component---src-pages-articles-2009-07-29-first-post-manifesto-mdx-37fc3db3c6459b875c05.js"],"component---src-pages-articles-2009-07-29-paris-game-ai-conference-09-mdx":["/component---src-pages-articles-2009-07-29-paris-game-ai-conference-09-mdx-e582ba3da357d7843da1.js"],"component---src-pages-articles-2009-08-12-siggraph-09-index-mdx":["/component---src-pages-articles-2009-08-12-siggraph-09-index-mdx-7ecc87ce48ed4efb1019.js"],"component---src-pages-articles-2009-09-18-minkowski-sum-index-mdx":["/component---src-pages-articles-2009-09-18-minkowski-sum-index-mdx-66186699efc3b383de28.js"],"component---src-pages-articles-2009-10-01-report-autodesk-virtual-entertainment-conference-index-mdx":["/component---src-pages-articles-2009-10-01-report-autodesk-virtual-entertainment-conference-index-mdx-4a2222cfbe904f844e18.js"],"component---src-pages-articles-2009-11-29-crowds-control-has-moved-mdx":["/component---src-pages-articles-2009-11-29-crowds-control-has-moved-mdx-3127eff9d7941b591c74.js"],"component---src-pages-articles-2009-11-30-aggregate-dynamics-for-dense-crowd-simulation-mdx":["/component---src-pages-articles-2009-11-30-aggregate-dynamics-for-dense-crowd-simulation-mdx-8fa77df3957dcc75c4cb.js"],"component---src-pages-articles-2010-02-15-std-vector-bool-is-not-a-std-vector-containing-bools-mdx":["/component---src-pages-articles-2010-02-15-std-vector-bool-is-not-a-std-vector-containing-bools-mdx-b4c5673c4cf4fbfb941a.js"],"component---src-pages-articles-2010-11-30-casa-2010-mdx":["/component---src-pages-articles-2010-11-30-casa-2010-mdx-b31cc436b144822493f5.js"],"component---src-pages-articles-2010-12-01-on-crowd-simulation-validation-index-mdx":["/component---src-pages-articles-2010-12-01-on-crowd-simulation-validation-index-mdx-9be69f0717fbe7ab22e7.js"],"component---src-pages-articles-2010-12-10-the-most-relevant-entry-point-on-computationnal-geometry-i-know-index-mdx":["/component---src-pages-articles-2010-12-10-the-most-relevant-entry-point-on-computationnal-geometry-i-know-index-mdx-16747ff0ac760149e239.js"],"component---src-pages-articles-2011-01-03-founders-at-work-testimonials-from-the-history-of-it-index-mdx":["/component---src-pages-articles-2011-01-03-founders-at-work-testimonials-from-the-history-of-it-index-mdx-a0e365b83b4285caee0a.js"],"component---src-pages-articles-2011-02-25-artificial-intelligence-for-games-by-ian-millington-index-mdx":["/component---src-pages-articles-2011-02-25-artificial-intelligence-for-games-by-ian-millington-index-mdx-54cb02bb7ae4f357df26.js"],"component---src-pages-articles-2011-04-09-golaem-crowds-launch-index-mdx":["/component---src-pages-articles-2011-04-09-golaem-crowds-launch-index-mdx-64ef0d6a099bd99ca718.js"],"component---src-pages-articles-2011-05-08-group-navigation-state-of-the-art-report-part-1-mdx":["/component---src-pages-articles-2011-05-08-group-navigation-state-of-the-art-report-part-1-mdx-7e2190687d87028b801d.js"],"component---src-pages-articles-2011-05-19-group-navigation-state-of-the-art-report-part-2-mdx":["/component---src-pages-articles-2011-05-19-group-navigation-state-of-the-art-report-part-2-mdx-2dd3dac898532f67cdeb.js"],"component---src-pages-articles-2011-06-14-how-simple-rules-determine-pedestrian-behavior-and-crowd-disasters-index-mdx":["/component---src-pages-articles-2011-06-14-how-simple-rules-determine-pedestrian-behavior-and-crowd-disasters-index-mdx-a4fa9ea4f1a04abd0f0d.js"],"component---src-pages-articles-2011-06-19-group-navigation-state-of-the-art-report-part-3-mdx":["/component---src-pages-articles-2011-06-19-group-navigation-state-of-the-art-report-part-3-mdx-c7793b8af237b8f30690.js"],"component---src-pages-articles-2011-06-24-towards-directable-autonomous-crowds-index-mdx":["/component---src-pages-articles-2011-06-24-towards-directable-autonomous-crowds-index-mdx-5e0f0e6ed03a6d698c03.js"],"component---src-pages-articles-2011-08-10-autonomous-agents-nursery-mdx":["/component---src-pages-articles-2011-08-10-autonomous-agents-nursery-mdx-3cf11574e5bdde1adad8.js"],"component---src-pages-articles-2011-08-21-learning-webgl-mdx":["/component---src-pages-articles-2011-08-21-learning-webgl-mdx-3db13362a52711cac908.js"],"component---src-pages-articles-2011-08-30-hosted-webgl-toy-mdx":["/component---src-pages-articles-2011-08-30-hosted-webgl-toy-mdx-17db3408d3e21d51ae55.js"],"component---src-pages-articles-2011-09-15-airport-hall-populated-with-golaem-crowd-mdx":["/component---src-pages-articles-2011-09-15-airport-hall-populated-with-golaem-crowd-mdx-50d3cb6c1425b5645e22.js"],"component---src-pages-articles-2011-10-16-changes-mdx":["/component---src-pages-articles-2011-10-16-changes-mdx-b0998b9edbdee814407f.js"],"component---src-pages-articles-2011-11-08-ml-class-index-mdx":["/component---src-pages-articles-2011-11-08-ml-class-index-mdx-3b36f6dfbe84d900f694.js"],"component---src-pages-articles-2011-11-09-formation-sketching-index-mdx":["/component---src-pages-articles-2011-11-09-formation-sketching-index-mdx-027ac8be422a3301d4b3.js"],"component---src-pages-articles-2012-05-12-yet-another-migration-mdx":["/component---src-pages-articles-2012-05-12-yet-another-migration-mdx-25e158812fcda1ed7fb6.js"],"component---src-pages-articles-2012-05-21-boudica-last-stand-index-mdx":["/component---src-pages-articles-2012-05-21-boudica-last-stand-index-mdx-e114a3eeb7abe0ea90ec.js"],"component---src-pages-articles-2013-03-21-recast-detour-in-masa-life-index-mdx":["/component---src-pages-articles-2013-03-21-recast-detour-in-masa-life-index-mdx-69b66a198ba0e30e01d3.js"],"component---src-pages-articles-2013-04-24-ai-summit-2013-takeaway-index-mdx":["/component---src-pages-articles-2013-04-24-ai-summit-2013-takeaway-index-mdx-585cad7ab2f0ba99d183.js"],"component---src-pages-articles-2013-08-13-a-sample-ios-application-using-ogre-3-d-index-mdx":["/component---src-pages-articles-2013-08-13-a-sample-ios-application-using-ogre-3-d-index-mdx-f00febea06955831d35f.js"],"component---src-pages-articles-2013-10-08-contributions-to-recastdetour-mdx":["/component---src-pages-articles-2013-10-08-contributions-to-recastdetour-mdx-cc68f2ceacf0d376bd8c.js"],"component---src-pages-articles-2014-03-17-gdcaisummit-2014-environmentally-conscious-ai-index-mdx":["/component---src-pages-articles-2014-03-17-gdcaisummit-2014-environmentally-conscious-ai-index-mdx-3a9dfd5dbfcced6e7685.js"],"component---src-pages-articles-2015-06-10-hierarchical-architecture-for-group-navigation-behaviors-index-mdx":["/component---src-pages-articles-2015-06-10-hierarchical-architecture-for-group-navigation-behaviors-index-mdx-f707214a0c2e4e73cb2f.js"],"component---src-pages-articles-2017-03-09-orange-hello-business-talks-mdx":["/component---src-pages-articles-2017-03-09-orange-hello-business-talks-mdx-bec1a722c5232274aa29.js"],"component---src-pages-articles-2020-03-04-ifttd-mdx":["/component---src-pages-articles-2020-03-04-ifttd-mdx-69a9c658364a88ac2748.js"],"component---src-pages-articles-2020-04-22-the-three-stages-of-explainable-ai-index-mdx":["/component---src-pages-articles-2020-04-22-the-three-stages-of-explainable-ai-index-mdx-86dcf7b7a4cd776d1ec9.js"],"component---src-pages-articles-2020-11-06-ai-and-biases-101-the-bakery-case-index-mdx":["/component---src-pages-articles-2020-11-06-ai-and-biases-101-the-bakery-case-index-mdx-66ed1eaa8455dba630fb.js"],"component---src-pages-articles-2021-05-04-when-ai-meets-simulation-mdx":["/component---src-pages-articles-2021-05-04-when-ai-meets-simulation-mdx-51aab55fcc34ef8937f3.js"],"component---src-pages-articles-index-js":["/component---src-pages-articles-index-js-bcec8970454fad3e45ba.js"],"component---src-pages-content-about-mdx":["/component---src-pages-content-about-mdx-0d356b5dd149c185f6a2.js"],"component---src-pages-content-certifications-index-mdx":["/component---src-pages-content-certifications-index-mdx-b44e53be037ae2097e8e.js"],"component---src-pages-content-resume-en-mdx":["/component---src-pages-content-resume-en-mdx-288926f2c831b694457d.js"],"component---src-pages-content-resume-fr-mdx":["/component---src-pages-content-resume-fr-mdx-09ecb58dc146c98686db.js"],"component---src-pages-index-js":["/component---src-pages-index-js-881ecf0edfec32e55e9b.js"]};/*]]>*/</script><script src="/component---src-pages-articles-2020-04-22-the-three-stages-of-explainable-ai-index-mdx-86dcf7b7a4cd776d1ec9.js" async=""></script><script src="/46ee4902551c23666229271b7c847a0d0f0f7703-1762214d0be939521d9b.js" async=""></script><script src="/0f1ac474-a5e16e9b990849dc8ee5.js" async=""></script><script src="/app-ad690ddceb3d8eaa4fb6.js" async=""></script><script src="/2b7b2d2a-4f51378a4c7bbaec6330.js" async=""></script><script src="/cb1608f2-8480d875f23d193a8efc.js" async=""></script><script src="/a9a7754c-2bf3d99a1f1ef2138d89.js" async=""></script><script src="/styles-39e9c908b573a9761a76.js" async=""></script><script src="/framework-ee697c05f43c773aa564.js" async=""></script><script src="/webpack-runtime-28fbd69803a77b26dfd6.js" async=""></script></body></html>